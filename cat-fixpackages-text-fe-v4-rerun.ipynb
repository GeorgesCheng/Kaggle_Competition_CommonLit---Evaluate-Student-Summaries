{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\"","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:43:10.472867Z","iopub.execute_input":"2023-09-19T09:43:10.47382Z","iopub.status.idle":"2023-09-19T09:43:45.724455Z","shell.execute_reply.started":"2023-09-19T09:43:10.473773Z","shell.execute_reply":"2023-09-19T09:43:45.723141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/pip-install-textstat-mit/package\")\nimport textstat","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:43:45.727401Z","iopub.execute_input":"2023-09-19T09:43:45.72781Z","iopub.status.idle":"2023-09-19T09:43:45.775119Z","shell.execute_reply.started":"2023-09-19T09:43:45.72777Z","shell.execute_reply":"2023-09-19T09:43:45.774008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/pip-install-autocorrect-mit/package\")\nimport autocorrect","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:43:45.77683Z","iopub.execute_input":"2023-09-19T09:43:45.777167Z","iopub.status.idle":"2023-09-19T09:43:45.810894Z","shell.execute_reply.started":"2023-09-19T09:43:45.777135Z","shell.execute_reply":"2023-09-19T09:43:45.810028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feedback feature","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:43:45.815364Z","iopub.execute_input":"2023-09-19T09:43:45.816092Z","iopub.status.idle":"2023-09-19T09:43:45.821928Z","shell.execute_reply.started":"2023-09-19T09:43:45.816057Z","shell.execute_reply":"2023-09-19T09:43:45.820979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\nfrom transformers import DataCollatorWithPadding\n%env TOKENIZERS_PARALLELISM=True\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\nimport textstat","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:43:45.823677Z","iopub.execute_input":"2023-09-19T09:43:45.824206Z","iopub.status.idle":"2023-09-19T09:43:59.140949Z","shell.execute_reply.started":"2023-09-19T09:43:45.824174Z","shell.execute_reply":"2023-09-19T09:43:59.139897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG1:\n    model = \"microsoft/deberta-v3-base\"\n    path = \"../input/0911-deberta-v3-base/\"\n    base = \"../input/fb3models/microsoft-deberta-v3-base/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=24\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=8\n    weight = 1.0\n    \nCFG_list = [CFG1]","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:43:59.142357Z","iopub.execute_input":"2023-09-19T09:43:59.144126Z","iopub.status.idle":"2023-09-19T09:43:59.541127Z","shell.execute_reply.started":"2023-09-19T09:43:59.144089Z","shell.execute_reply":"2023-09-19T09:43:59.540124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_mcrmse(eval_pred):\n    \"\"\"\n    Calculates mean columnwise root mean squared error\n    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n    \"\"\"\n    preds, labels = eval_pred\n\n    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n    mcrmse = np.mean(col_rmse)\n\n    return {\n        \"content_rmse\": col_rmse[0],\n        \"wording_rmse\": col_rmse[1],\n        \"mcrmse\": mcrmse,\n    }","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:43:59.542723Z","iopub.execute_input":"2023-09-19T09:43:59.543088Z","iopub.status.idle":"2023-09-19T09:43:59.550732Z","shell.execute_reply.started":"2023-09-19T09:43:59.543053Z","shell.execute_reply":"2023-09-19T09:43:59.549511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FROM OLD \n# ====================================================\n# Utils\n# ====================================================\ndef MCRMSE(y_trues, y_preds):\n    scores = []\n    idxes = y_trues.shape[1]\n    for i in range(idxes):\n        y_true = y_trues[:,i]\n        y_pred = y_preds[:,i]\n        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n        scores.append(score)\n    mcrmse_score = np.mean(scores)\n    return mcrmse_score, scores\n\ndef get_score(y_trues, y_preds):\n    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n    return mcrmse_score, scores\n\ndef get_logger(filename='inference'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)\n\n# ====================================================\n# oof\n# ====================================================\nfor CFG in CFG_list:\n    oof_df = pd.read_pickle(CFG.path+'oof_df.pkl')\n    labels = oof_df[CFG.target_cols].values\n    preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n    score, scores = get_score(labels, preds)\n    LOGGER.info(f'Model: {CFG.model} Score: {score:<.4f}  Scores: {scores}')\n    \n# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(cfg, text):\n    inputs = cfg.tokenizer.encode_plus(\n        text, \n        return_tensors=None, \n        add_special_tokens=True, \n        #max_length=CFG.max_len,\n        #pad_to_max_length=True,\n        #truncation=True\n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['full_text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n\n# new for CL\nclass CommonlitDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n    \n# ====================================================\n# Model\n# ====================================================\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\nclass MaxPooling(nn.Module):\n    def __init__(self):\n        super(MaxPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = -1e4\n        max_embeddings, _ = torch.max(embeddings, dim = 1)\n        return max_embeddings\n    \nclass MinPooling(nn.Module):\n    def __init__(self):\n        super(MinPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = 1e-4\n        min_embeddings, _ = torch.min(embeddings, dim = 1)\n        return min_embeddings\n        \n\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n            self.config.hidden_dropout = 0.\n            self.config.hidden_dropout_prob = 0.\n            self.config.attention_dropout = 0.\n            self.config.attention_probs_dropout_prob = 0.\n            LOGGER.info(self.config)\n        else:\n            self.config = AutoConfig.from_pretrained(config_path, output_hidden_states=True)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        if self.cfg.gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n        self.pool = MeanPooling()\n        self.fc = nn.Linear(self.config.hidden_size, 6)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(feature)\n        return output\n    \n# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in test_loader:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:43:59.552631Z","iopub.execute_input":"2023-09-19T09:43:59.552986Z","iopub.status.idle":"2023-09-19T09:43:59.72176Z","shell.execute_reply.started":"2023-09-19T09:43:59.552953Z","shell.execute_reply":"2023-09-19T09:43:59.720556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts_test = pd.read_csv(\"/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv\")\nsummaries_test = pd.read_csv(\"/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/commonlit-evaluate-student-summaries/sample_submission.csv\")\ntest = summaries_test.merge(prompts_test, how=\"left\", on=\"prompt_id\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:43:59.723375Z","iopub.execute_input":"2023-09-19T09:43:59.723728Z","iopub.status.idle":"2023-09-19T09:43:59.756498Z","shell.execute_reply.started":"2023-09-19T09:43:59.723693Z","shell.execute_reply":"2023-09-19T09:43:59.755159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create some features using text statistics and use classic ML algorithm","metadata":{}},{"cell_type":"code","source":"#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('/kaggle/input/sentence-transformers/minilm-l6-v2/all-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained('/kaggle/input/sentence-transformers/minilm-l6-v2/all-MiniLM-L6-v2')\nMEMORY = {}\n\ndef get_emb(sentences):\n    if sentences in MEMORY:\n        return MEMORY[sentences]\n    # Tokenize sentences\n    encoded_input = tokenizer([sentences], padding=True, truncation=True, return_tensors='pt')\n\n    # Compute token embeddings\n    with torch.no_grad():\n        model_output = model(**encoded_input)\n\n    # Perform pooling\n    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n    # Normalize embeddings\n    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)[0].detach().cpu().numpy()\n    MEMORY[sentences] = sentence_embeddings\n    \n    return sentence_embeddings","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:43:59.758385Z","iopub.execute_input":"2023-09-19T09:43:59.758778Z","iopub.status.idle":"2023-09-19T09:44:02.355935Z","shell.execute_reply.started":"2023-09-19T09:43:59.75874Z","shell.execute_reply":"2023-09-19T09:44:02.35485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_feedback_feat(data):\n    for _idx, CFG in enumerate(CFG_list):\n        dataset = CommonlitDataset(CFG, data)\n        loader = DataLoader(dataset,\n                            batch_size=CFG.batch_size,\n                            shuffle=False,\n                            collate_fn=DataCollatorWithPadding(tokenizer=CFG.tokenizer, padding='longest'),\n                            num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n\n        predictions = []\n        for fold in CFG.trn_fold:\n            print('='*10, f'started fold {fold}', '='*10)\n            model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n            state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                               map_location=torch.device('cpu'))\n            model.load_state_dict(state['model'])\n            prediction = inference_fn(loader, model, device)\n            predictions.append(prediction)\n            torch.cuda.empty_cache()\n        predictions = np.mean(predictions, axis=0)\n        data[CFG.target_cols] = predictions\n        torch.cuda.empty_cache() \n        return data","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:44:02.361206Z","iopub.execute_input":"2023-09-19T09:44:02.361525Z","iopub.status.idle":"2023-09-19T09:44:02.37235Z","shell.execute_reply.started":"2023-09-19T09:44:02.361499Z","shell.execute_reply":"2023-09-19T09:44:02.371149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\ntqdm.pandas()\ntest_fe = get_feedback_feat(test)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:44:02.374007Z","iopub.execute_input":"2023-09-19T09:44:02.374932Z","iopub.status.idle":"2023-09-19T09:45:51.61292Z","shell.execute_reply.started":"2023-09-19T09:44:02.374903Z","shell.execute_reply":"2023-09-19T09:45:51.611724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#deberta feature","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:45:51.615015Z","iopub.execute_input":"2023-09-19T09:45:51.615421Z","iopub.status.idle":"2023-09-19T09:45:51.620406Z","shell.execute_reply.started":"2023-09-19T09:45:51.615374Z","shell.execute_reply":"2023-09-19T09:45:51.61931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List\nimport logging\nimport shutil\nimport json\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\nfrom transformers import DataCollatorWithPadding\nfrom datasets import Dataset,load_dataset, load_from_disk\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_metric, disable_progress_bar\nfrom sklearn.metrics import mean_squared_error\nimport torch\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom collections import Counter\nimport spacy\nimport re\nfrom autocorrect import Speller\nfrom spellchecker import SpellChecker\nimport lightgbm as lgb\n\nwarnings.simplefilter(\"ignore\")\nlogging.disable(logging.ERROR)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \ndisable_progress_bar()\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:45:51.621974Z","iopub.execute_input":"2023-09-19T09:45:51.622611Z","iopub.status.idle":"2023-09-19T09:45:56.339236Z","shell.execute_reply.started":"2023-09-19T09:45:51.622578Z","shell.execute_reply":"2023-09-19T09:45:56.33823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed: int):\n    import random, os\n    import numpy as np\n    import torch\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(seed=42)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:45:56.340787Z","iopub.execute_input":"2023-09-19T09:45:56.34109Z","iopub.status.idle":"2023-09-19T09:45:56.476206Z","shell.execute_reply.started":"2023-09-19T09:45:56.341051Z","shell.execute_reply":"2023-09-19T09:45:56.475044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    model_name=\"debertav3base\"\n    learning_rate=1.5e-5\n    weight_decay=0.02\n    hidden_dropout_prob=0.007\n    attention_probs_dropout_prob=0.007\n    num_train_epochs=5\n    n_splits=4\n    batch_size=12\n    random_seed=42\n    save_steps=100\n    max_length=512","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:45:56.478024Z","iopub.execute_input":"2023-09-19T09:45:56.479088Z","iopub.status.idle":"2023-09-19T09:45:56.489069Z","shell.execute_reply.started":"2023-09-19T09:45:56.478947Z","shell.execute_reply":"2023-09-19T09:45:56.48803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self, \n                model_name: str,\n                ) -> None:  \n        self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/{model_name}\")\n        self.twd = TreebankWordDetokenizer()\n        self.STOP_WORDS = set(stopwords.words('english'))\n        \n        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n        self.speller = Speller(lang='en')\n        self.spellchecker = SpellChecker() \n        \n    def word_overlap_count(self, row): \n        \"\"\" intersection(prompt_text, text) \"\"\"        \n        def check_is_stop_word(word):\n            return word in self.STOP_WORDS\n        \n        prompt_words = row['prompt_tokens']\n        summary_words = row['summary_tokens']\n        if self.STOP_WORDS:\n            prompt_words = list(filter(check_is_stop_word, prompt_words))\n            summary_words = list(filter(check_is_stop_word, summary_words))\n        return len(set(prompt_words).intersection(set(summary_words)))\n            \n    def ngrams(self, token, n): \n        # Use the zip function to help us generate n-grams\n        # Concatentate the tokens into ngrams and return\n        ngrams = zip(*[token[i:] for i in range(n)])\n        return [\" \".join(ngram) for ngram in ngrams]\n\n    def ngram_co_occurrence(self, row, n: int) -> int: \n        # Tokenize the original text and summary into words\n        original_tokens = row['prompt_tokens']\n        summary_tokens = row['summary_tokens']\n\n        # Generate n-grams for the original text and summary\n        original_ngrams = set(self.ngrams(original_tokens, n))\n        summary_ngrams = set(self.ngrams(summary_tokens, n))\n\n        # Calculate the number of common n-grams\n        common_ngrams = original_ngrams.intersection(summary_ngrams)\n        return len(common_ngrams)\n    \n    def ner_overlap_count(self, row, mode:str): \n        model = self.spacy_ner_model\n        def clean_ners(ner_list):\n            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n        prompt = model(row['prompt_text'])\n        summary = model(row['text'])\n\n        if \"spacy\" in str(model):\n            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n        elif \"stanza\" in str(model):\n            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n            summary_ner = set([(token.text, token.type) for token in summary.ents])\n        else:\n            raise Exception(\"Model not supported\")\n\n        prompt_ner = clean_ners(prompt_ner)\n        summary_ner = clean_ners(summary_ner)\n\n        intersecting_ners = prompt_ner.intersection(summary_ner)\n        \n        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n        \n        if mode == \"train\":\n            return ner_dict\n        elif mode == \"test\":\n            return {key: ner_dict.get(key) for key in self.ner_keys}\n\n    \n    def quotes_count(self, row): \n        summary = row['text']\n        text = row['prompt_text']\n        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n        if len(quotes_from_summary)>0:\n            return [quote in text for quote in quotes_from_summary].count(True)\n        else:\n            return 0\n\n    def spelling(self, text): \n        \n        wordlist=text.split()\n        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n\n        return amount_miss\n    \n    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]: \n        \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n        self.spellchecker.word_frequency.load_words(tokens)\n        self.speller.nlp_data.update({token:1000 for token in tokens})\n    \n    def run(self, \n            prompts: pd.DataFrame,\n            summaries:pd.DataFrame,\n            mode:str\n        ) -> pd.DataFrame: \n        \n        # before merge preprocess\n        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n            lambda x: len(word_tokenize(x))\n        )\n        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n\n        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n            lambda x: len(word_tokenize(x))\n        )\n        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n        \n        # Add prompt tokens into spelling checker dictionary\n        prompts[\"prompt_tokens\"].apply(\n            lambda x: self.add_spelling_dictionary(x)\n        )\n        \n#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n        # fix misspelling\n        summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n            lambda x: self.speller(x.lower())\n        )\n        \n        # count misspelling\n        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n        \n        # merge prompts and summaries\n        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n\n        # after merge preprocess\n        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n        \n        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n        input_df['bigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence,args=(2,), axis=1 \n        )\n        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n        \n        input_df['trigram_overlap_count'] = input_df.progress_apply(\n            self.ngram_co_occurrence, args=(3,), axis=1\n        )\n        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n        \n        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n        \n        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n    \npreprocessor = Preprocessor(model_name=CFG.model_name)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:45:56.490866Z","iopub.execute_input":"2023-09-19T09:45:56.491144Z","iopub.status.idle":"2023-09-19T09:46:00.033266Z","shell.execute_reply.started":"2023-09-19T09:45:56.491109Z","shell.execute_reply":"2023-09-19T09:46:00.032222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:46:00.034997Z","iopub.execute_input":"2023-09-19T09:46:00.035281Z","iopub.status.idle":"2023-09-19T09:46:00.182107Z","shell.execute_reply.started":"2023-09-19T09:46:00.035214Z","shell.execute_reply":"2023-09-19T09:46:00.18098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model Function Definition\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    rmse = mean_squared_error(labels, predictions, squared=False)\n    return {\"rmse\": rmse}\n\ndef compute_mcrmse(eval_pred):\n    \"\"\"\n    Calculates mean columnwise root mean squared error\n    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n    \"\"\"\n    preds, labels = eval_pred\n\n    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n    mcrmse = np.mean(col_rmse)\n\n    return {\n        \"content_rmse\": col_rmse[0],\n        \"wording_rmse\": col_rmse[1],\n        \"mcrmse\": mcrmse,\n    }\n\ndef compt_score(content_true, content_pred, wording_true, wording_pred):\n    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n    \n    return (content_score + wording_score)/2","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:46:00.183929Z","iopub.execute_input":"2023-09-19T09:46:00.184234Z","iopub.status.idle":"2023-09-19T09:46:00.193873Z","shell.execute_reply.started":"2023-09-19T09:46:00.184194Z","shell.execute_reply":"2023-09-19T09:46:00.192545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ContentScoreRegressor:\n    def __init__(self, \n                model_name: str,\n                model_dir: str,\n                target: str,\n                hidden_dropout_prob: float,\n                attention_probs_dropout_prob: float,\n                max_length: int,\n                ):\n        self.inputs = [\"prompt_text\", \"prompt_title\", \"prompt_question\", \"fixed_summary_text\"]\n        self.input_col = \"input\"\n        \n        self.text_cols = [self.input_col] \n        self.target = target\n        self.target_cols = [target]\n\n        self.model_name = model_name\n        self.model_dir = model_dir\n        self.max_length = max_length\n        \n        self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/{model_name}\")\n        self.model_config = AutoConfig.from_pretrained(f\"/kaggle/input/{model_name}\")\n        \n        self.model_config.update({\n            \"hidden_dropout_prob\": hidden_dropout_prob,\n            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n            \"num_labels\": 1,\n            \"problem_type\": \"regression\",\n        })\n        \n        seed_everything(seed=42)\n\n        self.data_collator = DataCollatorWithPadding(\n            tokenizer=self.tokenizer\n        )\n\n\n    def tokenize_function(self, examples: pd.DataFrame):\n        labels = [examples[self.target]]\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.max_length)\n        return {\n            **tokenized,\n            \"labels\": labels,\n        }\n    \n    def tokenize_function_test(self, examples: pd.DataFrame):\n        tokenized = self.tokenizer(examples[self.input_col],\n                         padding=False,\n                         truncation=True,\n                         max_length=self.max_length)\n        return tokenized\n        \n    def train(self, \n            fold: int,\n            train_df: pd.DataFrame,\n            valid_df: pd.DataFrame,\n            batch_size: int,\n            learning_rate: float,\n            weight_decay: float,\n            num_train_epochs: float,\n            save_steps: int,\n        ) -> None:\n        \"\"\"fine-tuning\"\"\"\n        \n        sep = self.tokenizer.sep_token\n        train_df[self.input_col] = (\n                    train_df[\"prompt_title\"] + sep \n                    + train_df[\"prompt_question\"] + sep \n                    + train_df[\"fixed_summary_text\"]\n                  )\n\n        valid_df[self.input_col] = (\n                    valid_df[\"prompt_title\"] + sep \n                    + valid_df[\"prompt_question\"] + sep \n                    + valid_df[\"fixed_summary_text\"]\n                  )\n        \n        train_df = train_df[[self.input_col] + self.target_cols]\n        valid_df = valid_df[[self.input_col] + self.target_cols]\n        \n        model_content = AutoModelForSequenceClassification.from_pretrained(\n            f\"/kaggle/input/{self.model_name}\", \n            config=self.model_config\n        )\n\n        train_dataset = Dataset.from_pandas(train_df, preserve_index=False) \n        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False) \n    \n        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=False)\n        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=False)\n\n        # eg. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n        \n        training_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            load_best_model_at_end=True, # select best model\n            learning_rate=learning_rate,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=8,\n            num_train_epochs=num_train_epochs,\n            weight_decay=weight_decay,\n            report_to='none',\n            greater_is_better=False,\n            save_strategy=\"steps\",\n            evaluation_strategy=\"steps\",\n            eval_steps=save_steps,\n            save_steps=save_steps,\n            metric_for_best_model=\"rmse\",\n            save_total_limit=1\n        )\n\n        trainer = Trainer(\n            model=model_content,\n            args=training_args,\n            train_dataset=train_tokenized_datasets,\n            eval_dataset=val_tokenized_datasets,\n            tokenizer=self.tokenizer,\n            compute_metrics=compute_metrics,\n            data_collator=self.data_collator\n        )\n\n        trainer.train()\n        \n        model_content.save_pretrained(self.model_dir)\n        self.tokenizer.save_pretrained(self.model_dir)\n\n        \n    def predict(self, \n                test_df: pd.DataFrame,\n                fold: int,\n               ):\n        \"\"\"predict content score\"\"\"\n        \n        sep = self.tokenizer.sep_token\n        in_text = (\n                    test_df[\"prompt_title\"] + sep \n                    + test_df[\"prompt_question\"] + sep \n                    + test_df[\"fixed_summary_text\"]\n                  )\n        test_df[self.input_col] = in_text\n\n        test_ = test_df[[self.input_col]]\n    \n        test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n\n        model_content = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n        model_content.eval()\n        \n        # e.g. \"bert/fold_0/\"\n        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n\n        test_args = TrainingArguments(\n            output_dir=model_fold_dir,\n            do_train = False,\n            do_predict = True,\n            per_device_eval_batch_size = 4,   \n            dataloader_drop_last = False,\n        )\n\n        # init trainer\n        infer_content = Trainer(\n                      model = model_content, \n                      tokenizer=self.tokenizer,\n                      data_collator=self.data_collator,\n                      args = test_args)\n\n        preds = infer_content.predict(test_tokenized_dataset)[0]\n\n        return preds","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:46:00.195868Z","iopub.execute_input":"2023-09-19T09:46:00.196824Z","iopub.status.idle":"2023-09-19T09:46:00.229263Z","shell.execute_reply.started":"2023-09-19T09:46:00.19675Z","shell.execute_reply":"2023-09-19T09:46:00.227963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_by_fold(\n        train_df: pd.DataFrame,\n        model_name: str,\n        target:str,\n        save_each_model: bool,\n        n_splits: int,\n        batch_size: int,\n        learning_rate: int,\n        hidden_dropout_prob: float,\n        attention_probs_dropout_prob: float,\n        weight_decay: float,\n        num_train_epochs: int,\n        save_steps: int,\n        max_length:int\n    ):\n\n    # delete old model files\n    if os.path.exists(model_name):\n        shutil.rmtree(model_name)\n    \n    os.mkdir(model_name)\n        \n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        train_data = train_df[train_df[\"fold\"] != fold]\n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        if save_each_model == True:\n            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{model_name}/fold_{fold}\"\n\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        csr.train(\n            fold=fold,\n            train_df=train_data,\n            valid_df=valid_data, \n            batch_size=batch_size,\n            learning_rate=learning_rate,\n            weight_decay=weight_decay,\n            num_train_epochs=num_train_epochs,\n            save_steps=save_steps,\n        )\n\ndef validate(\n    train_df: pd.DataFrame,\n    target:str,\n    save_each_model: bool,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ) -> pd.DataFrame:\n    \"\"\"predict oof data\"\"\"\n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        valid_data = train_df[train_df[\"fold\"] == fold]\n        \n        if save_each_model == True:\n            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{model_name}/fold_{fold}\"\n        \n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir,\n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        pred = csr.predict(\n            test_df=valid_data, \n            fold=fold\n        )\n        \n        train_df.loc[valid_data.index, f\"{target}_pred\"] = pred\n\n    return train_df","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:46:00.232631Z","iopub.execute_input":"2023-09-19T09:46:00.233297Z","iopub.status.idle":"2023-09-19T09:46:00.248756Z","shell.execute_reply.started":"2023-09-19T09:46:00.233238Z","shell.execute_reply":"2023-09-19T09:46:00.247659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#content model","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:46:00.252322Z","iopub.execute_input":"2023-09-19T09:46:00.252539Z","iopub.status.idle":"2023-09-19T09:46:00.265485Z","shell.execute_reply.started":"2023-09-19T09:46:00.252509Z","shell.execute_reply":"2023-09-19T09:46:00.264196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(\n    test_df: pd.DataFrame,\n    target:str,\n    save_each_model: bool,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ):\n    \"\"\"predict using mean folds\"\"\"\n\n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        if save_each_model == True:\n            model_dir =  f\"/kaggle/input/deberta-content-training-fix-autocorrect/{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{model_name}/fold_{fold}\"\n\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        pred = csr.predict(\n            test_df=test_df, \n            fold=fold\n        )\n        \n        test_df[f\"{target}_pred_{fold}\"] = pred\n    \n    test_df[f\"{target}_pred\"] = test_df[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n\n    return test_df","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:46:00.267443Z","iopub.execute_input":"2023-09-19T09:46:00.267726Z","iopub.status.idle":"2023-09-19T09:46:00.279454Z","shell.execute_reply.started":"2023-09-19T09:46:00.267694Z","shell.execute_reply":"2023-09-19T09:46:00.278567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for target in [\"content\"]:\n    test_content = predict(\n        test,\n        target=target,\n        save_each_model=True,\n        model_name=CFG.model_name,\n        hidden_dropout_prob=CFG.hidden_dropout_prob,\n        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n        max_length=CFG.max_length\n    )","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:46:00.281128Z","iopub.execute_input":"2023-09-19T09:46:00.281431Z","iopub.status.idle":"2023-09-19T09:46:35.938833Z","shell.execute_reply.started":"2023-09-19T09:46:00.281397Z","shell.execute_reply":"2023-09-19T09:46:35.93781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_content","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:46:35.940524Z","iopub.execute_input":"2023-09-19T09:46:35.940808Z","iopub.status.idle":"2023-09-19T09:46:35.972052Z","shell.execute_reply.started":"2023-09-19T09:46:35.940759Z","shell.execute_reply":"2023-09-19T09:46:35.970848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_content=test_content[['student_id', 'prompt_id','summary_length',\n                           'splling_err_num','prompt_length','length_ratio',\n                           'word_overlap_count','bigram_overlap_count','bigram_overlap_ratio',\n                           'trigram_overlap_count','trigram_overlap_ratio','quotes_count','content_pred']]","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:46:35.973662Z","iopub.execute_input":"2023-09-19T09:46:35.974107Z","iopub.status.idle":"2023-09-19T09:46:35.982806Z","shell.execute_reply.started":"2023-09-19T09:46:35.97407Z","shell.execute_reply":"2023-09-19T09:46:35.980537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_content=test_content.merge(test_fe,how='inner',on=['student_id', 'prompt_id'])","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:46:35.985099Z","iopub.execute_input":"2023-09-19T09:46:35.985881Z","iopub.status.idle":"2023-09-19T09:46:35.999786Z","shell.execute_reply.started":"2023-09-19T09:46:35.985841Z","shell.execute_reply":"2023-09-19T09:46:35.99855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wording model","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:46:36.001949Z","iopub.execute_input":"2023-09-19T09:46:36.002497Z","iopub.status.idle":"2023-09-19T09:46:36.014217Z","shell.execute_reply.started":"2023-09-19T09:46:36.002451Z","shell.execute_reply":"2023-09-19T09:46:36.007521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(\n    test_df: pd.DataFrame,\n    target:str,\n    save_each_model: bool,\n    model_name: str,\n    hidden_dropout_prob: float,\n    attention_probs_dropout_prob: float,\n    max_length : int\n    ):\n    \"\"\"predict using mean folds\"\"\"\n\n    for fold in range(CFG.n_splits):\n        print(f\"fold {fold}:\")\n        \n        if save_each_model == True:\n            model_dir =  f\"/kaggle/input/deberta-wording-training-fix-autocorrect/{target}/{model_name}/fold_{fold}\"\n        else: \n            model_dir =  f\"{model_name}/fold_{fold}\"\n\n        csr = ContentScoreRegressor(\n            model_name=model_name,\n            target=target,\n            model_dir = model_dir, \n            hidden_dropout_prob=hidden_dropout_prob,\n            attention_probs_dropout_prob=attention_probs_dropout_prob,\n            max_length=max_length,\n           )\n        \n        pred = csr.predict(\n            test_df=test_df, \n            fold=fold\n        )\n        \n        test_df[f\"{target}_pred_{fold}\"] = pred\n    \n    test_df[f\"{target}_pred\"] = test_df[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n\n    return test_df","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:46:36.017002Z","iopub.execute_input":"2023-09-19T09:46:36.018239Z","iopub.status.idle":"2023-09-19T09:46:36.034767Z","shell.execute_reply.started":"2023-09-19T09:46:36.017972Z","shell.execute_reply":"2023-09-19T09:46:36.031689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for target in [\"wording\"]:\n    test_wording = predict(\n        test,\n        target=target,\n        save_each_model=True,\n        model_name=CFG.model_name,\n        hidden_dropout_prob=CFG.hidden_dropout_prob,\n        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n        max_length=CFG.max_length\n    )","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:46:36.040198Z","iopub.execute_input":"2023-09-19T09:46:36.041674Z","iopub.status.idle":"2023-09-19T09:47:12.501914Z","shell.execute_reply.started":"2023-09-19T09:46:36.041626Z","shell.execute_reply":"2023-09-19T09:47:12.50071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_wording=test_wording[['student_id', 'prompt_id','wording_pred']]","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:12.503485Z","iopub.execute_input":"2023-09-19T09:47:12.504403Z","iopub.status.idle":"2023-09-19T09:47:12.511719Z","shell.execute_reply.started":"2023-09-19T09:47:12.504354Z","shell.execute_reply":"2023-09-19T09:47:12.510589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_wording","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:12.521358Z","iopub.execute_input":"2023-09-19T09:47:12.521575Z","iopub.status.idle":"2023-09-19T09:47:12.534597Z","shell.execute_reply.started":"2023-09-19T09:47:12.521549Z","shell.execute_reply":"2023-09-19T09:47:12.533448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=test_content.merge(test_wording,how='inner',on=['student_id', 'prompt_id'])","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:12.536834Z","iopub.execute_input":"2023-09-19T09:47:12.537607Z","iopub.status.idle":"2023-09-19T09:47:12.550607Z","shell.execute_reply.started":"2023-09-19T09:47:12.537513Z","shell.execute_reply":"2023-09-19T09:47:12.549319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# statistics","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:12.552875Z","iopub.execute_input":"2023-09-19T09:47:12.553279Z","iopub.status.idle":"2023-09-19T09:47:12.57176Z","shell.execute_reply.started":"2023-09-19T09:47:12.553214Z","shell.execute_reply":"2023-09-19T09:47:12.563876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nimport os\nimport glob\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nimport optuna\nfrom optuna.samplers import TPESampler\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\n\n\nimport string\nimport pickle\nimport operator\nfrom textblob import TextBlob\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom tqdm import tqdm\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:12.574415Z","iopub.execute_input":"2023-09-19T09:47:12.574699Z","iopub.status.idle":"2023-09-19T09:47:12.79934Z","shell.execute_reply.started":"2023-09-19T09:47:12.574662Z","shell.execute_reply":"2023-09-19T09:47:12.798167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model from HuggingFace Hub\ntokenizer_sb = AutoTokenizer.from_pretrained('/kaggle/input/sentence-transformers/sentence-transformers/minil2-l12-v2/paraphrase-multilingual-MiniLM-L12-v2')\nmodel_sb = AutoModel.from_pretrained('/kaggle/input/sentence-transformers/sentence-transformers/minil2-l12-v2/paraphrase-multilingual-MiniLM-L12-v2').cuda()\nMEMORY_SB = {}\n\ntokenizer_qa = AutoTokenizer.from_pretrained('/kaggle/input/multi-qa-mpnet-base-dot-v1/multi-qa-mpnet-base-dot-v1')\nmodel_qa = AutoModel.from_pretrained('/kaggle/input/multi-qa-mpnet-base-dot-v1/multi-qa-mpnet-base-dot-v1').cuda()\nMEMORY_QA = {}\n\ntokenizer_ch = AutoTokenizer.from_pretrained('/kaggle/input/bert-cohesion/bert_cohesion')\nmodel_ch = AutoModelForSequenceClassification.from_pretrained('/kaggle/input/bert-cohesion/bert_cohesion', num_labels=9).cuda()\n\ntokenizer_rw = AutoTokenizer.from_pretrained('/kaggle/input/reward-model-deberta-v3-large-v2-wch/reward-model-deberta-v3-large-v2-wch')\nmodel_rw = AutoModelForSequenceClassification.from_pretrained('/kaggle/input/reward-model-deberta-v3-large-v2-wch/reward-model-deberta-v3-large-v2-wch',\n                                                              num_labels=1).cuda()\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\ndef cls_pooling(model_output):\n    return model_output.last_hidden_state[:,0]\n\ndef get_emb_sb(sentences):\n    if sentences in MEMORY_SB:\n        return MEMORY_SB[sentences]\n    # Tokenize sentences\n    encoded_input = tokenizer_sb([sentences], padding=True, truncation=True, return_tensors='pt')\n    encoded_input = {k:v.cuda() for k, v in encoded_input.items()}\n\n    # Compute token embeddings\n    with torch.no_grad():\n        model_output = model_sb(**encoded_input)\n\n    # Perform pooling\n    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n    # Normalize embeddings\n    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)[0].detach().cpu().numpy()\n    MEMORY_SB[sentences] = sentence_embeddings\n    \n    return sentence_embeddings\n\ndef get_emb_qa(sentences):\n    if sentences in MEMORY_QA:\n        return MEMORY_QA[sentences]\n    # Tokenize sentences\n    encoded_input = tokenizer_qa([sentences], padding=True, truncation=True, return_tensors='pt')\n    encoded_input = {k:v.cuda() for k, v in encoded_input.items()}\n    \n    # Compute token embeddings\n    with torch.no_grad():\n        model_output = model_qa(**encoded_input)\n\n    # Perform pooling\n    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n    # Normalize embeddings\n    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)[0].detach().cpu().numpy()\n    MEMORY_QA[sentences] = sentence_embeddings\n    \n    return sentence_embeddings\n\ndef get_ch_label(sentences):\n    encoded_input = tokenizer_ch([sentences], padding=True, truncation=True, return_tensors='pt')\n    encoded_input = {k:v.cuda() for k, v in encoded_input.items()}\n    \n    with torch.no_grad():\n        model_output = model_ch(**encoded_input)[\"logits\"]\n    \n#     print(model_output)\n    label = torch.argmax(model_output, dim=1)[0].detach().cpu().numpy()\n    \n    return label\n\ndef get_rw(question, answer):\n    encoded_input = tokenizer_rw(question, answer, padding=True, truncation=True, return_tensors='pt')\n    encoded_input = {k:v.cuda() for k, v in encoded_input.items()}\n    \n    with torch.no_grad():\n        model_output = model_rw(**encoded_input)[\"logits\"]\n    \n    return torch.sigmoid(model_output)[0].cpu().detach().numpy()[0]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:12.800711Z","iopub.execute_input":"2023-09-19T09:47:12.801036Z","iopub.status.idle":"2023-09-19T09:47:47.764703Z","shell.execute_reply.started":"2023-09-19T09:47:12.800987Z","shell.execute_reply":"2023-09-19T09:47:47.763572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pos_count(sent):\n    nn_count = 0   #Noun\n    pr_count = 0   #Pronoun\n    vb_count = 0   #Verb\n    jj_count = 0   #Adjective\n    uh_count = 0   #Interjection\n    cd_count = 0   #Numerics\n    \n    sent = nltk.word_tokenize(sent)\n    sent = nltk.pos_tag(sent)\n\n    for token in sent:\n        if token[1] in ['NN','NNP','NNS']:\n            nn_count += 1\n\n        if token[1] in ['PRP','PRP$']:\n            pr_count += 1\n\n        if token[1] in ['VB','VBD','VBG','VBN','VBP','VBZ']:\n            vb_count += 1\n\n        if token[1] in ['JJ','JJR','JJS']:\n            jj_count += 1\n\n        if token[1] in ['UH']:\n            uh_count += 1\n\n        if token[1] in ['CD']:\n            cd_count += 1\n    \n    return pd.Series([nn_count, pr_count, vb_count, jj_count, uh_count, cd_count])","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:47.7663Z","iopub.execute_input":"2023-09-19T09:47:47.766653Z","iopub.status.idle":"2023-09-19T09:47:47.78391Z","shell.execute_reply.started":"2023-09-19T09:47:47.766616Z","shell.execute_reply":"2023-09-19T09:47:47.77732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"num_words\"] = test[\"text\"].progress_apply(lambda x: len(str(x).split()))\ntest[\"num_unique_words\"] = test[\"text\"].progress_apply(lambda x: len(set(str(x).split())))\ntest[\"num_chars\"] = test[\"text\"].progress_apply(lambda x: len(str(x)))\ntest[\"num_stopwords\"] = test[\"text\"].progress_apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\ntest[\"num_punctuations\"] =test['text'].progress_apply(lambda x: len([c for c in str(x) if c in list(string.punctuation)]))\ntest[\"num_words_upper\"] = test[\"text\"].progress_apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"num_words_title\"] = test[\"text\"].progress_apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest[\"mean_word_len\"] = test[\"text\"].progress_apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest[\"mean_num_unique_words\"] = test[\"text\"].progress_apply(lambda x: np.mean([len(set(e.split())) for e in x.split('.')]))\ntest[\"num_paragraphs\"] = test[\"text\"].progress_apply(lambda x: len(x.split('\\n\\n')))\ntest[\"num_slash\"] = test[\"text\"].progress_apply(lambda x: len(x.split('\\n')))\ntest[\"syntax_count\"] = test[\"text\"].progress_apply(lambda x: x.count(\",\") + x.count(\"-\") + x.count(\";\") + x.count(\":\"))\ntest[\"num_sentences\"] = test[\"text\"].progress_apply(lambda x: len(str(x).split('.')))\ntest[\"polarity\"] = test['text'].progress_apply(lambda x: TextBlob(x).sentiment[0])\ntest[\"subjectivity\"] = test['text'].progress_apply(lambda x: TextBlob(x).sentiment[1])\ntest[['nn_count','pr_count','vb_count','jj_count','uh_count','cd_count']] = test['text'].progress_apply(pos_count)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:47.790305Z","iopub.execute_input":"2023-09-19T09:47:47.79351Z","iopub.status.idle":"2023-09-19T09:47:48.172836Z","shell.execute_reply.started":"2023-09-19T09:47:47.793455Z","shell.execute_reply":"2023-09-19T09:47:48.17173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_stat_features(df, text_col=\"text\"):\n    df['diff_emb_sb'] = df.progress_apply(lambda x: np.sum(get_emb_sb(x[\"text\"])*get_emb_sb(x[\"prompt_text\"])), axis=1)\n    df['diff_emb_qa'] = df.progress_apply(lambda x: np.sum(get_emb_qa(x[\"text\"])*get_emb_qa(x[\"prompt_question\"])), axis=1)\n    df['ch'] = df.progress_apply(lambda x: get_ch_label(x[\"text\"]), axis=1)\n    df['rw'] = df.progress_apply(lambda x: get_rw(x[\"prompt_question\"], x[\"text\"]), axis=1)\n\n    df['automated_readability_index'] = df[text_col].progress_apply(lambda x: textstat.automated_readability_index(x))\n    df['coleman_liau_index'] = df[text_col].progress_apply(lambda x: textstat.coleman_liau_index(x))\n    df['smog_index'] = df[text_col].progress_apply(lambda x: textstat.smog_index(x))\n    df['flesch_reading_ease'] = df[text_col].progress_apply(lambda x: textstat.flesch_reading_ease(x))\n    df['flesch_kincaid_grade'] = df[text_col].progress_apply(lambda x: textstat.flesch_kincaid_grade(x))\n    df['dale_chall_readability_score'] = df[text_col].progress_apply(lambda x: textstat.dale_chall_readability_score(x))\n    df['linsear_write_formula'] = df[text_col].progress_apply(lambda x: textstat.linsear_write_formula(x))\n    df['gunning_fog'] = df[text_col].progress_apply(lambda x: textstat.gunning_fog(x))\n    df['text_standard_float'] = df[text_col].progress_apply(lambda x: textstat.text_standard(x, float_output=True))\n    df['spache_readability'] = df[text_col].progress_apply(lambda x: textstat.spache_readability(x))\n    df['rix'] = df[text_col].progress_apply(lambda x: textstat.rix(x))\n    df['lix'] = df[text_col].progress_apply(lambda x: textstat.lix(x))\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:48.17445Z","iopub.execute_input":"2023-09-19T09:47:48.17482Z","iopub.status.idle":"2023-09-19T09:47:48.187896Z","shell.execute_reply.started":"2023-09-19T09:47:48.174751Z","shell.execute_reply":"2023-09-19T09:47:48.186916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test= get_stat_features(test)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:48.189729Z","iopub.execute_input":"2023-09-19T09:47:48.190349Z","iopub.status.idle":"2023-09-19T09:47:48.684298Z","shell.execute_reply.started":"2023-09-19T09:47:48.190309Z","shell.execute_reply":"2023-09-19T09:47:48.683469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk import pos_tag\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom collections import Counter\nfrom nltk import ne_chunk, word_tokenize, pos_tag\nimport dask.dataframe as dd\nfrom dask.multiprocessing import get","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:48.685524Z","iopub.execute_input":"2023-09-19T09:47:48.685938Z","iopub.status.idle":"2023-09-19T09:47:48.705394Z","shell.execute_reply.started":"2023-09-19T09:47:48.685897Z","shell.execute_reply":"2023-09-19T09:47:48.704705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Additional features\n\ndef calculate_pos_ratios(text):\n    pos_tags = pos_tag(nltk.word_tokenize(text))\n    pos_counts = Counter(tag for word, tag in pos_tags)\n    total_words = len(pos_tags)\n    ratios = {tag: count / total_words for tag, count in pos_counts.items()}\n    return ratios\n\nsid = SentimentIntensityAnalyzer()\ndef calculate_sentiment_scores(text):\n    sentiment_scores = sid.polarity_scores(text)\n    return sentiment_scores\n \n\ndef calculate_punctuation_ratios(text):\n    total_chars = len(text)\n    punctuation_counts = Counter(char for char in text if char in '.,!?;:\"()[]{}')\n    ratios = {char: count / total_chars for char, count in punctuation_counts.items()}\n    return ratios\n\n\n# keyword density\ndef calculate_keyword_density(row):\n    keywords = set(row['prompt_text'].split())\n    text_words = row['text'].split()\n    keyword_count = sum(1 for word in text_words if word in keywords)\n    return keyword_count / len(text_words)\n\n\n# Function to calculate sentiment scores for prompt text\nsid = SentimentIntensityAnalyzer()\ndef calculate_sentiment_scores_prompt(text):\n    sentiment_scores = sid.polarity_scores(text)\n    return sentiment_scores","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:48.708116Z","iopub.execute_input":"2023-09-19T09:47:48.708379Z","iopub.status.idle":"2023-09-19T09:47:48.745073Z","shell.execute_reply.started":"2023-09-19T09:47:48.708349Z","shell.execute_reply":"2023-09-19T09:47:48.743726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_col=['pos_ratios','sentiment_scores','punctuation_ratios','sentiment_scores_prompt']","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:48.746188Z","iopub.execute_input":"2023-09-19T09:47:48.746447Z","iopub.status.idle":"2023-09-19T09:47:48.753652Z","shell.execute_reply.started":"2023-09-19T09:47:48.746412Z","shell.execute_reply":"2023-09-19T09:47:48.751808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['comma_count'] = test['text'].apply(lambda x: x.count(','))\ntest['semicolon_count'] = test['text'].apply(lambda x: x.count(';'))\ntest['exclamation_count'] = test['text'].apply(lambda x: x.count('!'))\ntest['question_count'] = test['text'].apply(lambda x: x.count('?'))\n# Calculate POS ratios for each row\ntest['pos_ratios'] = test['text'].apply(calculate_pos_ratios)\n\n# Convert the dictionary of POS ratios into a single value (mean)\ntest['pos_mean'] = test['pos_ratios'].apply(lambda x: np.mean(list(x.values())))\n \n#Calculate sentiment scores for each row\ntest['sentiment_scores'] = test['text'].apply(calculate_sentiment_scores)\n\n# Convert sentiment_scores into individual columns\nsentiment_columns = pd.DataFrame(list(test['sentiment_scores']))\ntest= pd.concat([test, sentiment_columns], axis=1)\n\ntest['punctuation_ratios'] = test['text'].apply(calculate_punctuation_ratios)\n\n# Convert the dictionary of punctuation ratios into a single value (sum)\ntest['punctuation_sum'] = test['punctuation_ratios'].apply(lambda x: np.sum(list(x.values())))\n\ntest['keyword_density'] = test.apply(calculate_keyword_density, axis=1)\n\n\n# Calculate sentiment scores for prompt text\ntest['sentiment_scores_prompt'] = test['prompt_text'].apply(calculate_sentiment_scores_prompt)\n\n# Convert sentiment_scores_prompt into individual columns\nsentiment_columns_prompt = pd.DataFrame(list(test['sentiment_scores_prompt']))\nsentiment_columns_prompt.columns = [col +'_prompt' for col in sentiment_columns_prompt.columns]\n\ntest = pd.concat([test, sentiment_columns_prompt], axis=1)\n\n# Jaccard similarity between prompt text and text\ntest['jaccard_similarity'] = test.apply(lambda row: len(set(word_tokenize(row['prompt_text'])) & set(word_tokenize(row['text']))) / len(set(word_tokenize(row['prompt_text'])) | set(word_tokenize(row['text']))), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:48.755128Z","iopub.execute_input":"2023-09-19T09:47:48.755583Z","iopub.status.idle":"2023-09-19T09:47:48.792869Z","shell.execute_reply.started":"2023-09-19T09:47:48.75555Z","shell.execute_reply":"2023-09-19T09:47:48.791971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.drop(drop_col, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:48.795173Z","iopub.execute_input":"2023-09-19T09:47:48.795696Z","iopub.status.idle":"2023-09-19T09:47:48.80221Z","shell.execute_reply.started":"2023-09-19T09:47:48.795659Z","shell.execute_reply":"2023-09-19T09:47:48.801034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom textblob import TextBlob","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:48.80414Z","iopub.execute_input":"2023-09-19T09:47:48.804487Z","iopub.status.idle":"2023-09-19T09:47:48.816479Z","shell.execute_reply.started":"2023-09-19T09:47:48.804448Z","shell.execute_reply":"2023-09-19T09:47:48.815221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self, \n                model_name: str,\n                ) -> None:\n        self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/{model_name}\")\n        self.twd = TreebankWordDetokenizer()\n        self.STOP_WORDS = set(stopwords.words('english'))\n        \n        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n        self.speller = Speller(lang='en')\n        self.spellchecker = SpellChecker() \n    \n        \n    def calculate_text_similarity(self, row):\n        vectorizer = TfidfVectorizer()\n        tfidf_matrix = vectorizer.fit_transform([row['prompt_text'], row['text']])\n        return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2]).flatten()[0]\n    \n    \n    def run(self, \n            prompts: pd.DataFrame,\n            summaries:pd.DataFrame,\n            mode:str\n        ) -> pd.DataFrame:\n        \n        # before merge preprocess\n\n                # before merge preprocess\n        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n\n        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n            lambda x: word_tokenize(x)\n        )\n        \n        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n        tqdm.pandas(desc=\"Calculating Text Similarity\")\n        input_df['text_similarity'] = input_df.progress_apply(self.calculate_text_similarity, axis=1)\n\n        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n    \npreprocessor = Preprocessor(model_name=CFG.model_name)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:48.818303Z","iopub.execute_input":"2023-09-19T09:47:48.818796Z","iopub.status.idle":"2023-09-19T09:47:51.675401Z","shell.execute_reply.started":"2023-09-19T09:47:48.818755Z","shell.execute_reply":"2023-09-19T09:47:51.674393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_2 = preprocessor.run(prompts_test, summaries_test, mode=\"test\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:51.677063Z","iopub.execute_input":"2023-09-19T09:47:51.677325Z","iopub.status.idle":"2023-09-19T09:47:51.723602Z","shell.execute_reply.started":"2023-09-19T09:47:51.67729Z","shell.execute_reply":"2023-09-19T09:47:51.722403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_2=test_2[['student_id', 'prompt_id','text_similarity']]","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:51.725401Z","iopub.execute_input":"2023-09-19T09:47:51.726021Z","iopub.status.idle":"2023-09-19T09:47:51.732705Z","shell.execute_reply.started":"2023-09-19T09:47:51.72596Z","shell.execute_reply":"2023-09-19T09:47:51.731652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=test.merge(test_2,how='inner',on=['student_id', 'prompt_id'])","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:51.734689Z","iopub.execute_input":"2023-09-19T09:47:51.735378Z","iopub.status.idle":"2023-09-19T09:47:51.750153Z","shell.execute_reply.started":"2023-09-19T09:47:51.735339Z","shell.execute_reply":"2023-09-19T09:47:51.748949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['ch']=test['ch'].astype(float)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:51.752476Z","iopub.execute_input":"2023-09-19T09:47:51.752796Z","iopub.status.idle":"2023-09-19T09:47:51.763893Z","shell.execute_reply.started":"2023-09-19T09:47:51.752759Z","shell.execute_reply":"2023-09-19T09:47:51.762853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.columns","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:51.767755Z","iopub.execute_input":"2023-09-19T09:47:51.767982Z","iopub.status.idle":"2023-09-19T09:47:51.778684Z","shell.execute_reply.started":"2023-09-19T09:47:51.767949Z","shell.execute_reply":"2023-09-19T09:47:51.777701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/train-final-fe-fixpackages/train_final_fe_fixpakcages.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:51.780002Z","iopub.execute_input":"2023-09-19T09:47:51.780747Z","iopub.status.idle":"2023-09-19T09:47:52.511787Z","shell.execute_reply.started":"2023-09-19T09:47:51.780705Z","shell.execute_reply":"2023-09-19T09:47:52.510787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:52.514314Z","iopub.execute_input":"2023-09-19T09:47:52.514984Z","iopub.status.idle":"2023-09-19T09:47:52.5221Z","shell.execute_reply.started":"2023-09-19T09:47:52.51495Z","shell.execute_reply":"2023-09-19T09:47:52.521084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_features = [\"student_id\", \"prompt_id\", \"prompt_question\", \"prompt_title\", \"prompt_text\",'fold','fixed_summary_text','text']\ntarget = [\"content\", \"wording\"]\nfe_columns = [col for col in train.columns if col not in no_features + target]\n\ncorr = train[fe_columns + target].corr()\ncorr.style.background_gradient(cmap='coolwarm')","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:52.523562Z","iopub.execute_input":"2023-09-19T09:47:52.524497Z","iopub.status.idle":"2023-09-19T09:47:53.217942Z","shell.execute_reply.started":"2023-09-19T09:47:52.524456Z","shell.execute_reply":"2023-09-19T09:47:53.217081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fe_columns=['summary_length',\n 'splling_err_num',\n 'prompt_length',\n 'length_ratio',\n 'word_overlap_count',\n 'bigram_overlap_count',\n 'bigram_overlap_ratio',\n 'trigram_overlap_count',\n 'trigram_overlap_ratio',\n 'quotes_count',\n 'content_pred',\n 'cohesion',\n 'syntax',\n 'vocabulary',\n 'phraseology',\n 'grammar',\n 'conventions',\n 'wording_pred',\n 'num_words',\n 'num_unique_words',\n 'num_chars',\n 'num_stopwords',\n 'num_punctuations',\n 'num_words_upper',\n 'num_words_title',\n 'mean_word_len',\n 'mean_num_unique_words',\n 'num_paragraphs',\n 'num_slash',\n 'syntax_count',\n 'num_sentences',\n 'polarity',\n 'subjectivity',\n 'nn_count',\n 'pr_count',\n 'vb_count',\n 'jj_count',\n 'uh_count',\n 'cd_count',\n 'diff_emb_sb',\n 'diff_emb_qa',\n 'ch',\n 'rw',\n 'automated_readability_index',\n 'coleman_liau_index',\n 'smog_index',\n 'flesch_reading_ease',\n 'flesch_kincaid_grade',\n 'dale_chall_readability_score',\n 'linsear_write_formula',\n 'gunning_fog',\n 'text_standard_float',\n 'spache_readability',\n 'rix',\n 'lix',\n 'comma_count',\n 'semicolon_count',\n 'exclamation_count',\n 'question_count',\n 'pos_mean',\n 'neg',\n 'neu',\n 'pos',\n 'compound',\n 'punctuation_sum',\n 'keyword_density',\n 'neg_prompt',\n 'neu_prompt',\n 'pos_prompt',\n 'compound_prompt',\n 'jaccard_similarity',\n 'text_similarity','text']","metadata":{"execution":{"iopub.status.busy":"2023-09-19T09:47:53.219032Z","iopub.execute_input":"2023-09-19T09:47:53.219313Z","iopub.status.idle":"2023-09-19T09:47:53.231565Z","shell.execute_reply.started":"2023-09-19T09:47:53.219277Z","shell.execute_reply":"2023-09-19T09:47:53.230431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_list=['summary_length',\n 'splling_err_num',\n 'prompt_length',\n 'length_ratio',\n 'word_overlap_count',\n 'bigram_overlap_count',\n 'bigram_overlap_ratio',\n 'trigram_overlap_count',\n 'trigram_overlap_ratio',\n 'quotes_count',\n 'content_pred',\n 'cohesion',\n 'syntax',\n 'vocabulary',\n 'phraseology',\n 'grammar',\n 'conventions',\n 'wording_pred',\n 'num_words',\n 'num_unique_words',\n 'num_chars',\n 'num_stopwords',\n 'num_punctuations',\n 'num_words_upper',\n 'num_words_title',\n 'mean_word_len',\n 'mean_num_unique_words',\n 'num_paragraphs',\n 'num_slash',\n 'syntax_count',\n 'num_sentences',\n 'polarity',\n 'subjectivity',\n 'nn_count',\n 'pr_count',\n 'vb_count',\n 'jj_count',\n 'uh_count',\n 'cd_count',\n 'diff_emb_sb',\n 'diff_emb_qa',\n 'ch',\n 'rw',\n 'automated_readability_index',\n 'coleman_liau_index',\n 'smog_index',\n 'flesch_reading_ease',\n 'flesch_kincaid_grade',\n 'dale_chall_readability_score',\n 'linsear_write_formula',\n 'gunning_fog',\n 'text_standard_float',\n 'spache_readability',\n 'rix',\n 'lix',\n 'comma_count',\n 'semicolon_count',\n 'exclamation_count',\n 'question_count',\n 'pos_mean',\n 'neg',\n 'neu',\n 'pos',\n 'compound',\n 'punctuation_sum',\n 'keyword_density',\n 'neg_prompt',\n 'neu_prompt',\n 'pos_prompt',\n 'compound_prompt',\n 'jaccard_similarity',\n 'text_similarity']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = \"content\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfrom catboost import CatBoostRegressor, Pool","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = []\n\nfor fold in range(4):\n\n    X_train_cv = train[train[\"fold\"] != fold][fe_columns]\n    y_train_cv = train[train[\"fold\"] != fold][target]\n\n    X_eval_cv = train[train[\"fold\"] == fold][fe_columns]\n    y_eval_cv = train[train[\"fold\"] == fold][target]\n\n    model = CatBoostRegressor(\n        learning_rate = 0.048,\n        depth = 4,\n        min_data_in_leaf = 34,\n        iterations = 10000,\n        random_seed=42,\n        loss_function ='RMSE',\n        text_features=[\"text\"]\n      )\n    model.fit(X_train_cv, \n              y_train_cv, \n              eval_set=[(X_eval_cv, y_eval_cv)],\n              verbose=False)\n    models.append(model)\nrmses = []\npreds = []\ntrues = []\nfor fold, model in enumerate(models):\n    print(fold)\n    X_eval_cv = train[train[\"fold\"] == fold][fe_columns]\n    y_eval_cv = train[train[\"fold\"] == fold][target]\n\n    pred = model.predict(X_eval_cv)\n\n    trues.extend(y_eval_cv.values)\n    preds.extend(pred)\n\nrmse = np.sqrt(mean_squared_error(trues, preds))\nprint(rmse)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmse\nbest_features = fe_columns.copy()\nfor i in feature_list:\n    preds = []\n    trues = []\n    features = [x for x in best_features if x not in [i]]\n    print(features)\n    print('Evaluating {} column'.format(i))\n    for fold in range(4):\n        X_train_cv = train[train[\"fold\"] != fold][features]\n        y_train_cv = train[train[\"fold\"] != fold][target]\n\n        X_eval_cv = train[train[\"fold\"] == fold][features]\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        model = CatBoostRegressor(\n            learning_rate = 0.048,\n            depth = 4,\n            min_data_in_leaf = 34,\n            iterations = 10000,\n            random_seed=42,\n            loss_function ='RMSE',\n            text_features=[\"text\"]\n          )\n        model.fit(X_train_cv, \n                  y_train_cv, \n                  eval_set=[(X_eval_cv, y_eval_cv)],\n                  verbose=False)\n        pred = model.predict(X_eval_cv)\n        trues.extend(y_eval_cv.values)\n        preds.extend(pred)\n    loss_score = np.sqrt(mean_squared_error(trues, preds))\n    print(loss_score)\n    print(score)\n    print('loss_score is :', loss_score)\n    if loss_score < score:\n        print('Feature {} is useless'.format(i))\n        best_features.remove(i)\n        score = loss_score\n    else:\n        print('Feature {} is usefull'.format(i))\n    gc.collect()\nprint('The best features are: ', best_features )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"content_best_features=best_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = \"wording\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = []\n\nfor fold in range(4):\n\n    X_train_cv = train[train[\"fold\"] != fold][fe_columns]\n    y_train_cv = train[train[\"fold\"] != fold][target]\n\n    X_eval_cv = train[train[\"fold\"] == fold][fe_columns]\n    y_eval_cv = train[train[\"fold\"] == fold][target]\n\n    model = CatBoostRegressor(\n        learning_rate = 0.048,\n        depth = 4,\n        min_data_in_leaf = 34,\n        iterations = 10000,\n        random_seed=42,\n        loss_function ='RMSE',\n        text_features=[\"text\"]\n      )\n    model.fit(X_train_cv, \n              y_train_cv, \n              eval_set=[(X_eval_cv, y_eval_cv)],\n              verbose=False)\n    models.append(model)\nrmses = []\npreds = []\ntrues = []\nfor fold, model in enumerate(models):\n    print(fold)\n    X_eval_cv = train[train[\"fold\"] == fold][fe_columns]\n    y_eval_cv = train[train[\"fold\"] == fold][target]\n\n    pred = model.predict(X_eval_cv)\n\n    trues.extend(y_eval_cv.values)\n    preds.extend(pred)\n\nrmse = np.sqrt(mean_squared_error(trues, preds))\nprint(rmse)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmse\nbest_features = fe_columns.copy()\nfor i in feature_list:\n    preds = []\n    trues = []\n    features = [x for x in best_features if x not in [i]]\n    print(features)\n    print('Evaluating {} column'.format(i))\n    for fold in range(4):\n        X_train_cv = train[train[\"fold\"] != fold][features]\n        y_train_cv = train[train[\"fold\"] != fold][target]\n\n        X_eval_cv = train[train[\"fold\"] == fold][features]\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        model = CatBoostRegressor(\n            learning_rate = 0.048,\n            depth = 4,\n            min_data_in_leaf = 34,\n            iterations = 10000,\n            random_seed=42,\n            loss_function ='RMSE',\n            text_features=[\"text\"]\n          )\n        model.fit(X_train_cv, \n                  y_train_cv, \n                  eval_set=[(X_eval_cv, y_eval_cv)],\n                  verbose=False)\n        \n        pred = model.predict(X_eval_cv)\n        trues.extend(y_eval_cv.values)\n        preds.extend(pred)\n    loss_score = np.sqrt(mean_squared_error(trues, preds))\n    print(loss_score)\n    print(score)\n    print('loss_score is :', loss_score)\n    if loss_score < score:\n        print('Feature {} is useless'.format(i))\n        best_features.remove(i)\n        score = loss_score\n    else:\n        print('Feature {} is usefull'.format(i))\n    gc.collect()\nprint('The best features are: ', best_features )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wording_best_features=best_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_dic={}\nfeatures_dic['content']=content_best_features\nfeatures_dic['wording']=wording_best_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets=['content','wording']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dict = {}\n\nfor target in targets:\n    \n    features=features_dic['%s'%target]\n    print(features)\n    \n    models = []\n    \n    for fold in range(CFG.n_splits):\n\n        X_train_cv = train[train[\"fold\"] != fold][features]\n        y_train_cv = train[train[\"fold\"] != fold][target]\n\n        X_eval_cv = train[train[\"fold\"] == fold][features]\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        model = CatBoostRegressor(\n            learning_rate = 0.048,\n            depth = 4,\n            min_data_in_leaf = 34,\n            iterations = 10000,\n            random_seed=42,\n            loss_function ='RMSE',\n            text_features=[\"text\"]\n          )\n        model.fit(X_train_cv, \n                  y_train_cv, \n                  eval_set=[(X_eval_cv, y_eval_cv)],\n                  verbose=True)\n        \n        feature_importance = model.feature_importances_\n        sorted_idx = np.argsort(feature_importance)\n        fig = plt.figure(figsize=(12, 6))\n        plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n        plt.yticks(range(len(sorted_idx)), np.array(X_train_cv.columns)[sorted_idx])\n        plt.title('Feature Importance')\n        models.append(model)\n    model_dict[target] = models","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cv\nrmses = []\n\nfor target in targets:\n    \n    features=features_dic['%s'%target]\n    models = model_dict[target]\n\n    preds = []\n    trues = []\n    \n    for fold, model in enumerate(models):\n        X_eval_cv = train[train[\"fold\"] == fold][features]\n        y_eval_cv = train[train[\"fold\"] == fold][target]\n\n        pred = model.predict(X_eval_cv)\n\n        trues.extend(y_eval_cv)\n        preds.extend(pred)\n        \n    rmse = np.sqrt(mean_squared_error(trues, preds))\n    print(f\"{target}_rmse : {rmse}\")\n    rmses = rmses + [rmse]\n\nprint(f\"mcrmse : {sum(rmses) / len(rmses)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_dict = {}\nfor target in targets:\n    \n    features=features_dic['%s'%target]\n    \n    models = model_dict[target]\n    preds = []\n\n    for fold, model in enumerate(models):\n        X_eval_cv = test[features]\n\n        pred = model.predict(X_eval_cv)\n        preds.append(pred)\n    \n    pred_dict[target] = preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for target in targets:\n    preds = pred_dict[target]\n    for i, pred in enumerate(preds):\n        test[f\"{target}_pred_{i}\"] = pred\n\n    test[target] = test[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[[\"student_id\", \"content\", \"wording\"]].to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}