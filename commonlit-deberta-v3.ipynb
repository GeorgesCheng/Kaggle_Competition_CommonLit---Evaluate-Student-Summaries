{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport os\nimport gc\nfrom tqdm.auto import tqdm\nimport transformers\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nimport plotly.express as px #graphing\nimport plotly.graph_objects as go #graphing\nfrom plotly.subplots import make_subplots #graphing\nimport plotly.figure_factory as ff #graphing\nfrom torch.nn.parameter import Parameter\nfrom transformers import get_polynomial_decay_schedule_with_warmup,get_cosine_schedule_with_warmup,get_linear_schedule_with_warmup\nfrom transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\nfrom transformers import DataCollatorWithPadding,DataCollatorForTokenClassification\n\n\nfrom text_unidecode import unidecode\nfrom typing import Dict, List, Tuple\nimport codecs\nfrom datasets import concatenate_datasets,load_dataset,load_from_disk\n\nfrom sklearn.metrics import log_loss\n\nfrom transformers import AutoModel, AutoTokenizer, AdamW, DataCollatorWithPadding\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nimport time\nimport warnings\nimport collections\nfrom termcolor import colored\n\nfrom torch.optim import lr_scheduler\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoModel, AutoConfig, \n    AutoTokenizer, logging\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\n\n\nclass cfg:\n    select = 'large'\n    model_name = f'/kaggle/input/deberta-v3-{select}/deberta-v3-{select}'\n    only_model_name = f'deberta-v3-{select}'\n    accum_iter = 16\n    fold = 4\n    split = 5\n    seed = 42\n    batch_size = 2\n    max_len = 1024\n    num_epoch = 5\n    T_max= 500\n    \n    scheduler = 'CosineAnnealingLR'\n    weight_decay =  1e-6\n    min_lr = 1e-6\n    freezing = True\n    pooling = 'GemText'\n    weight_decay = 1e-2\n    encoder_lr = 1e-5\n    decoder_lr = 1e-5\n    eps = 1e-6\n    betas = (0.9, 0.999)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n\ndef MCRMSE(y_trues, y_preds):\n    scores = []\n    idxes = y_trues.shape[1]\n    for i in range(idxes):\n        y_true = y_trues[:,i]#.detach().to('cpu').numpy()\n        y_pred = y_preds[:,i]#.detach().to('cpu').numpy()\n        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n        scores.append(score)\n    mcrmse_score = np.mean(scores)\n    return mcrmse_score, scores\n\n\ndef score_loss(y_trues, y_preds):\n    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n    return {\n        'mcrmse_score' : mcrmse_score,\n        'Content_score' : scores[0],\n        'Wording_score' : scores[1]\n    }\n\ndef get_logger(filename='Training'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\n\ndef set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(cfg.seed)\n\n\nLOGGER.info(f\"=========================== Model name :{cfg.only_model_name} ===========================: \")\nLOGGER.info('\\n')\nLOGGER.info(f\"Scheduler: {cfg.scheduler}\")\nLOGGER.info(f\"batch_size: {cfg.batch_size} with gradient Accumukation {cfg.accum_iter} \")\nLOGGER.info(f\"Pooling name: {cfg.pooling} \")\nLOGGER.info(f\"Freezing: {cfg.freezing}\")\nLOGGER.info(f\"Max Length: {cfg.max_len}\")\nLOGGER.info(f\"Num Epochs: {cfg.num_epoch}\")\nLOGGER.info('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts_train = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_train.csv')\nprompts_test = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')\nsubmission = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/sample_submission.csv')\nsummary_train = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv')\nsummary_test = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\n\nprint(f\"Prompt Train.shape: {prompts_train.shape}\")\ndisplay(prompts_train.head())\nprint(f\"Summary Train.shape: {summary_train.shape}\")\ndisplay(summary_train.head())\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary_train.groupby(['prompt_id'])['content'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary_train.groupby(['prompt_id'])['wording'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = prompts_train.merge(summary_train, on=\"prompt_id\")\ntrain.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train['fold'] = -1\n# fold = StratifiedKFold(n_splits=cfg.fold, shuffle=True, random_state=cfg.seed)\n# for n, (train_index, val_index) in enumerate(fold.split(train, train['prompt_id'])):\n#     train.loc[val_index, 'fold'] = n\n# train['fold'] = train['fold'].astype(int)\n# fold_sizes = train.groupby('fold').size()\n# print(fold_sizes)\n\ngkf = GroupKFold(n_splits=4)\n\nfor i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n    train.loc[val_index, \"fold\"] = i\n    \nfold_sizes = train.groupby('fold').size()\nprint(fold_sizes)\n\ntrain.head()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_words_text = train[\"text\"].apply(lambda x: len(x.split())).max()\nmax_words_prompt_question = train[\"prompt_question\"].apply(lambda x: len(x.split())).max()\nmax_words_prompt_text = train[\"prompt_text\"].apply(lambda x: len(x.split())).max()\n\n## max words\nmax_words_text , max_words_prompt_question , max_words_prompt_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\ntokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\ncfg.tokenizer = tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg.tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pq = train['prompt_question'].values\ntext = train['text'].values\n\nfull_text = pq+\" \" + tokenizer.sep_token +\" \"+text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_text[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n        self.tokenizer = cfg.tokenizer\n        self.max_len = cfg.max_len\n        self.pq = df['prompt_question'].values\n        self.text = df['text'].values\n        self.targets = df[['content' , 'wording']].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self , index):\n        pq   =   self.pq[index]\n        text =   self.text[index]\n        full_text = pq+\" \" + self.tokenizer.sep_token +\" \"+text\n        \n        inputs = self.tokenizer.encode_plus(\n                        full_text,\n                        truncation=True,\n                        add_special_tokens=True,\n                        max_length=self.max_len,\n                        padding='max_length'\n                        \n                    )\n        \n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        target = self.targets[index]\n        \n   \n        return {\n            'input_ids': torch.tensor(ids, dtype=torch.long),\n            'attention_mask': torch.tensor(mask, dtype=torch.long),\n            \n        } , torch.tensor(target, dtype=torch.float)\n\ndef collate(inputs):\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n    for k, v in inputs.items():\n        inputs[k] = inputs[k][:, :mask_len]\n    return inputs\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MaxPooling(nn.Module):\n    def __init__(self):\n        super(MaxPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = -1e9\n        max_embeddings, _ = torch.max(embeddings, dim = 1)\n        return max_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MeanMax(nn.Module):\n    def __init__(self):\n        super(MeanMax, self).__init__()\n        \n        self.mean_pooler = MeanPooling()\n        self.max_pooler  = MaxPooling()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        mean_pooler = self.mean_pooler( last_hidden_state ,attention_mask )\n        max_pooler =  self.max_pooler( last_hidden_state ,attention_mask )\n        out = torch.concat([mean_pooler ,max_pooler ] , 1)\n        return out\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GeMText(nn.Module):\n    def __init__(self, dim = 1, p=3, eps=1e-6):\n        super(GeMText, self).__init__()\n        self.dim = dim\n        self.p = Parameter(torch.ones(1) * p)\n        self.eps = eps\n        self.feat_mult = 1\n\n    def forward(self, last_hidden_state, attention_mask):\n        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.shape)\n        x = (last_hidden_state.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n        ret = ret.pow(1 / self.p)\n        return ret","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_pooling_layer():\n    if cfg.pooling == 'Mean':\n        return MeanPooling()\n    \n    elif cfg.pooling == 'Max':\n        return MaxPooling()\n    \n    elif cfg.pooling == 'MeanMax':\n        return MeanMaxPooling()\n    \n    elif cfg.pooling == 'GemText':\n        return GeMText()\n\n\nprint(get_pooling_layer())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def freeze(module):\n    \"\"\"\n    Freezes module's parameters.\n    \"\"\"\n    \n    for parameter in module.parameters():\n        parameter.requires_grad = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def odd_layer_freeze(module):\n    for i in range(1,24,2):\n        for n,p in module.encoder.layer[i].named_parameters():\n            p.requires_grad = False\n            \ndef even_layer_freeze(module):\n    for i in range(0,24,2):\n        for n,p in module.encoder.layer[i].named_parameters():\n            p.requires_grad = False\n            \ndef top_half_layer_freeze(module):\n    for i in range(0,13,1):\n        for n,p in module.encoder.layer[i].named_parameters():\n            p.requires_grad = False\n\ndef bottom_half_layer_freeze(module):\n    for i in range(13,14,1):\n        for n,p in module.encoder.layer[i].named_parameters():\n            p.requires_grad = False\n            \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n'''\n## Check layers which one are freeze \nfor n,p in model.named_parameters():\n    print(n,p.requires_grad)\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#if cfg.freezing:\n#    top_half_layer_freeze(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BaselineModel(nn.Module):\n    def __init__(self, model_name ):\n        super(BaselineModel, self).__init__()\n        \n        self.model = AutoModel.from_pretrained(cfg.model_name)\n        self.config = AutoConfig.from_pretrained(cfg.model_name)\n        #self.drop = nn.Dropout(p=0.2)\n        self.pooler = get_pooling_layer()\n\n        if cfg.pooling == 'MeanMax':\n            self.fc = nn.Linear(2*self.config.hidden_size, 2)\n        else:\n            self.fc = nn.Linear(self.config.hidden_size, 2)\n            \n        \n        self._init_weights(self.fc)\n        \n        if cfg.freezing:\n            top_half_layer_freeze(self.model)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n           \n    def forward(self, ids, mask):\n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.pooler(out.last_hidden_state, mask)\n        #out = self.drop(out)\n        outputs = self.fc(out)\n        return outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_run(model ,criterion ,optimizer , dataloader):\n    \n    model.train()\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    running_loss = 0.0\n    dataset_size = 0.0 \n    \n    \n    for batch_idx , (data , labels) in bar:\n        inputs , target = collate(data) , labels    \n        ids  =  inputs['input_ids'].to(cfg.device, dtype = torch.long)\n        mask = inputs['attention_mask'].to(cfg.device, dtype = torch.long)\n        targets = target.to(cfg.device, dtype = torch.float)\n        \n        batch_size = ids.size(0)\n        outputs = model(ids, mask)\n        loss = criterion(outputs, targets)\n        \n        # normalize loss to account for batch accumulation\n        loss = loss / cfg.accum_iter \n        loss.backward()\n        \n        if ((batch_idx + 1) % cfg.accum_iter == 0) or (batch_idx + 1 == len(dataloader)):\n            optimizer.step()\n            optimizer.zero_grad()\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n\n    epoch_loss = running_loss/dataset_size\n    gc.collect()\n    \n\n    \n    return epoch_loss\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_run(model , dataloader):\n    model.eval()\n    \n    running_loss = 0.0\n    dataset_size = 0.0\n    \n    predictions = []\n    y_labels = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for batch_idx , (data , labels) in bar:\n        inputs , target = collate(data) , labels\n        ids  =  inputs['input_ids'].to(cfg.device, dtype = torch.long)\n        mask = inputs['attention_mask'].to(cfg.device, dtype = torch.long)\n        targets = target.to(cfg.device, dtype = torch.float)\n        \n        batch_size = ids.size(0)\n\n        outputs = model(ids, mask)\n        \n        loss = criterion(outputs, targets)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        predictions.append(outputs.detach().to('cpu').numpy())\n        y_labels.append(labels.detach().to('cpu').numpy())\n    \n    predictions = np.concatenate(predictions)\n    y_labels    = np.concatenate(y_labels)\n    epoch_loss = running_loss / dataset_size\n    gc.collect()   \n    \n    return epoch_loss , predictions , y_labels\n        \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_fold(fold):\n    \n    \n    dftrain = train[train['fold']!= n_fold]\n    dfvalid = train[train['fold']== n_fold]\n    \n    train_dataset = TrainDataset(dftrain)\n    valid_dataset = TrainDataset(dfvalid)\n    \n    train_loader = DataLoader(train_dataset , batch_size=cfg.batch_size ,num_workers=2, shuffle=True, pin_memory=True)\n    valid_loader = DataLoader(valid_dataset ,batch_size=cfg.batch_size,num_workers=2, shuffle=True, pin_memory=True)\n    \n    return train_loader , valid_loader\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def oof_df(n_fold , true , pred):\n    \n    df_pred = pd.DataFrame(pred ,columns= ['pred_content' , 'pred_wording'] )\n    df_real = pd.DataFrame(true ,columns= ['content' , 'wording'] )\n    \n    df = pd.concat([df_real , df_pred ],1)\n\n    \n    return df\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n              'lr': encoder_lr, 'weight_decay': weight_decay},\n            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n              'lr': encoder_lr, 'weight_decay': 0.0},\n            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n              'lr': decoder_lr, 'weight_decay': 0.0}\n        ]\n        return optimizer_parameters","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_dfs = []\nfor n_fold in range(cfg.fold):\n    LOGGER.info('\\n')\n    LOGGER.info(f\"========== fold: {n_fold} training ==========\")\n    train_loader, valid_loader = prepare_fold(fold=n_fold)\n    LOGGER.info(f'Number of batches in Train {len(train_loader) } and valid {len(valid_loader)} dataset')\n    model  = BaselineModel(cfg.model_name).to(cfg.device)   \n    optimizer_parameters = get_optimizer_params(model,\n                                                encoder_lr=cfg.encoder_lr, \n                                                decoder_lr=cfg.decoder_lr,\n                                                weight_decay=cfg.weight_decay)\n\n    optimizer = AdamW(optimizer_parameters, lr=cfg.encoder_lr, eps=cfg.eps, betas=cfg.betas)\n    scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.T_max, \n                                                   eta_min=cfg.min_lr)    \n    \n   \n    criterion = nn.SmoothL1Loss(reduction='mean')\n    \n    start = time.time()\n    best_epoch_score = np.inf\n    for epoch in range(cfg.num_epoch):\n        \n        train_loss  = train_run(model ,criterion ,optimizer , dataloader=train_loader)\n        valid_loss , valid_preds , valid_labels  = valid_run(model , dataloader=valid_loader)\n        \n        if valid_loss < best_epoch_score:\n            \n            LOGGER.info(f\"Validation Loss Improved ({best_epoch_score} ---> {valid_loss})\")\n            best_epoch_score = valid_loss\n            ### saving weights\n            torch.save(model.state_dict(), f\"{cfg.only_model_name}_Fold_{n_fold}.pth\") \n            \n            ## saving oof values\n            df_ = oof_df(n_fold , valid_labels , valid_preds)\n            \n            LOGGER.info(f'Weights and oof values saved for epochs-{epoch} .....')\n            \n        LOGGER.info(f\"Epoch {epoch} Training Loss {np.round(train_loss , 4)} Validation Loss {np.round(valid_loss , 4)}\")\n    \n        \n    end = time.time()\n    time_elapsed = end - start\n    \n    LOGGER.info(' Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    \n    LOGGER.info(\" Best Loss: {:.4f}\".format(best_epoch_score))\n    \n    oof_dfs.append(df_)            \n    LOGGER.info(f\" oof for fold {n_fold} ---> {score_loss(valid_labels, valid_preds )}\")\n    del model, train_loader, valid_loader , df_ , valid_preds , valid_labels\n    gc.collect()\n    LOGGER.info('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_df = pd.concat(oof_dfs , ignore_index=True )\noof_df.to_csv('oof_df.csv' , index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}