{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db13928",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-09-05T07:03:44.851457Z",
     "iopub.status.busy": "2023-09-05T07:03:44.851043Z",
     "iopub.status.idle": "2023-09-05T07:04:11.138830Z",
     "shell.execute_reply": "2023-09-05T07:04:11.137686Z"
    },
    "papermill": {
     "duration": 26.301632,
     "end_time": "2023-09-05T07:04:11.142299",
     "exception": false,
     "start_time": "2023-09-05T07:03:44.840667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/autocorrect/autocorrect-2.6.1.tar\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: autocorrect\r\n",
      "  Building wheel for autocorrect (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622363 sha256=83ab534591f74bad1601d80c3b06a7f9c50fae291373338da7c25154aa8c5d9b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/db/69/42/0fb0421d2fe70d195a04665edc760cfe5fd341d7bb8d8e0aaa\r\n",
      "Successfully built autocorrect\r\n",
      "Installing collected packages: autocorrect\r\n",
      "Successfully installed autocorrect-2.6.1\r\n",
      "Processing /kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\r\n",
      "Installing collected packages: pyspellchecker\r\n",
      "Successfully installed pyspellchecker-0.7.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"/kaggle/input/autocorrect/autocorrect-2.6.1.tar\"\n",
    "!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2064e273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T07:04:11.165041Z",
     "iopub.status.busy": "2023-09-05T07:04:11.164172Z",
     "iopub.status.idle": "2023-09-05T07:04:31.087405Z",
     "shell.execute_reply": "2023-09-05T07:04:31.086491Z"
    },
    "papermill": {
     "duration": 19.937023,
     "end_time": "2023-09-05T07:04:31.090037",
     "exception": false,
     "start_time": "2023-09-05T07:04:11.153014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset,load_dataset, load_from_disk\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric, disable_progress_bar\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import re\n",
    "from autocorrect import Speller\n",
    "from spellchecker import SpellChecker\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.disable(logging.ERROR)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "disable_progress_bar()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db99d8ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T07:04:31.107360Z",
     "iopub.status.busy": "2023-09-05T07:04:31.107130Z",
     "iopub.status.idle": "2023-09-05T07:04:31.117020Z",
     "shell.execute_reply": "2023-09-05T07:04:31.116215Z"
    },
    "papermill": {
     "duration": 0.020188,
     "end_time": "2023-09-05T07:04:31.118998",
     "exception": false,
     "start_time": "2023-09-05T07:04:31.098810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Random seed function\n",
    "# 随机种子函数\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ea0031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T07:04:31.133900Z",
     "iopub.status.busy": "2023-09-05T07:04:31.133705Z",
     "iopub.status.idle": "2023-09-05T07:04:31.138662Z",
     "shell.execute_reply": "2023-09-05T07:04:31.137709Z"
    },
    "papermill": {
     "duration": 0.015041,
     "end_time": "2023-09-05T07:04:31.141070",
     "exception": false,
     "start_time": "2023-09-05T07:04:31.126029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set configuration\n",
    "# 设置配置类\n",
    "class CFG:\n",
    "    model_name=\"debertav3base\"\n",
    "    learning_rate=1.5e-5\n",
    "    weight_decay=0.02\n",
    "    hidden_dropout_prob=0.007\n",
    "    attention_probs_dropout_prob=0.007\n",
    "    num_train_epochs=5\n",
    "    n_splits=4\n",
    "    batch_size=12\n",
    "    random_seed=42\n",
    "    save_steps=100\n",
    "    max_length=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21459b6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T07:04:31.155805Z",
     "iopub.status.busy": "2023-09-05T07:04:31.155614Z",
     "iopub.status.idle": "2023-09-05T07:04:31.176343Z",
     "shell.execute_reply": "2023-09-05T07:04:31.175539Z"
    },
    "papermill": {
     "duration": 0.030666,
     "end_time": "2023-09-05T07:04:31.178554",
     "exception": false,
     "start_time": "2023-09-05T07:04:31.147888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n",
    "prompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\n",
    "summaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\n",
    "sample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccf01922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T07:04:31.194573Z",
     "iopub.status.busy": "2023-09-05T07:04:31.194194Z",
     "iopub.status.idle": "2023-09-05T07:04:33.629987Z",
     "shell.execute_reply": "2023-09-05T07:04:33.629016Z"
    },
    "papermill": {
     "duration": 2.446869,
     "end_time": "2023-09-05T07:04:33.632403",
     "exception": false,
     "start_time": "2023-09-05T07:04:31.185534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 下面是比较重要的部分：数据预处理。\n",
    "class Preprocessor:\n",
    "    def __init__(self, \n",
    "                model_name: str,\n",
    "                ) -> None: #初始化方法，接受一个字符串参数 model_name，用于构建预处理器实例。在初始化过程中，它创建了许多预处理所需的工具，如 tokenizer、词汇表、停用词集合等。 \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/{model_name}\")\n",
    "        self.twd = TreebankWordDetokenizer()\n",
    "        self.STOP_WORDS = set(stopwords.words('english'))\n",
    "        \n",
    "        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n",
    "        self.speller = Speller(lang='en')\n",
    "        self.spellchecker = SpellChecker() \n",
    "        \n",
    "    def word_overlap_count(self, row): # 计算两个文本序列中词语的交集数量。使用 prompt_tokens 和 summary_tokens 字段中的词语。\n",
    "        \"\"\" intersection(prompt_text, text) \"\"\"        \n",
    "        def check_is_stop_word(word):\n",
    "            return word in self.STOP_WORDS\n",
    "        \n",
    "        prompt_words = row['prompt_tokens']\n",
    "        summary_words = row['summary_tokens']\n",
    "        if self.STOP_WORDS:\n",
    "            prompt_words = list(filter(check_is_stop_word, prompt_words))\n",
    "            summary_words = list(filter(check_is_stop_word, summary_words))\n",
    "        return len(set(prompt_words).intersection(set(summary_words)))\n",
    "            \n",
    "    def ngrams(self, token, n): # 生成文本序列的 n-gram。将输入的 token 列表分割为 n-gram，并返回 n-gram 列表.\n",
    "        # Use the zip function to help us generate n-grams\n",
    "        # Concatentate the tokens into ngrams and return\n",
    "        ngrams = zip(*[token[i:] for i in range(n)])\n",
    "        return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "    def ngram_co_occurrence(self, row, n: int) -> int: # 计算原始文本和摘要文本的 n-gram 共现数量。\n",
    "        # Tokenize the original text and summary into words\n",
    "        original_tokens = row['prompt_tokens']\n",
    "        summary_tokens = row['summary_tokens']\n",
    "\n",
    "        # Generate n-grams for the original text and summary\n",
    "        original_ngrams = set(self.ngrams(original_tokens, n))\n",
    "        summary_ngrams = set(self.ngrams(summary_tokens, n))\n",
    "\n",
    "        # Calculate the number of common n-grams\n",
    "        common_ngrams = original_ngrams.intersection(summary_ngrams)\n",
    "        return len(common_ngrams)\n",
    "    \n",
    "    def ner_overlap_count(self, row, mode:str): #计算两个文本中命名实体（Named Entity）的交集数量。使用 spacy 或 stanza 模型提取命名实体。\n",
    "        model = self.spacy_ner_model\n",
    "        def clean_ners(ner_list):\n",
    "            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n",
    "        prompt = model(row['prompt_text'])\n",
    "        summary = model(row['text'])\n",
    "\n",
    "        if \"spacy\" in str(model):\n",
    "            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n",
    "            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n",
    "        elif \"stanza\" in str(model):\n",
    "            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n",
    "            summary_ner = set([(token.text, token.type) for token in summary.ents])\n",
    "        else:\n",
    "            raise Exception(\"Model not supported\")\n",
    "\n",
    "        prompt_ner = clean_ners(prompt_ner)\n",
    "        summary_ner = clean_ners(summary_ner)\n",
    "\n",
    "        intersecting_ners = prompt_ner.intersection(summary_ner)\n",
    "        \n",
    "        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            return ner_dict\n",
    "        elif mode == \"test\":\n",
    "            return {key: ner_dict.get(key) for key in self.ner_keys}\n",
    "\n",
    "    \n",
    "    def quotes_count(self, row): #计算文本中引号的数量，用于检测文本中的引用。\n",
    "        summary = row['text']\n",
    "        text = row['prompt_text']\n",
    "        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n",
    "        if len(quotes_from_summary)>0:\n",
    "            return [quote in text for quote in quotes_from_summary].count(True)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def spelling(self, text): #计算文本中拼写错误的数量.\n",
    "        \n",
    "        wordlist=text.split()\n",
    "        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n",
    "\n",
    "        return amount_miss\n",
    "    \n",
    "    def add_spelling_dictionary(self, tokens: List[str]) -> List[str]: #将指定的单词列表添加到拼写检查的词典中。 \n",
    "        \"\"\"dictionary update for pyspell checker and autocorrect\"\"\"\n",
    "        self.spellchecker.word_frequency.load_words(tokens)\n",
    "        self.speller.nlp_data.update({token:1000 for token in tokens})\n",
    "    \n",
    "    def run(self, \n",
    "            prompts: pd.DataFrame,\n",
    "            summaries:pd.DataFrame,\n",
    "            mode:str\n",
    "        ) -> pd.DataFrame: #执行整个预处理流程。包括计算文本长度、拼写检查、特征生成等。\n",
    "        \n",
    "        # before merge preprocess\n",
    "        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n",
    "            lambda x: len(word_tokenize(x))\n",
    "        )\n",
    "        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n",
    "            lambda x: word_tokenize(x)\n",
    "        )\n",
    "\n",
    "        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n",
    "            lambda x: len(word_tokenize(x))\n",
    "        )\n",
    "        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n",
    "            lambda x: word_tokenize(x)\n",
    "        )\n",
    "        \n",
    "        # Add prompt tokens into spelling checker dictionary\n",
    "        prompts[\"prompt_tokens\"].apply(\n",
    "            lambda x: self.add_spelling_dictionary(x)\n",
    "        )\n",
    "        \n",
    "#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "        # fix misspelling\n",
    "        summaries[\"fixed_summary_text\"] = summaries[\"text\"].progress_apply(\n",
    "            lambda x: self.speller(x)\n",
    "        )\n",
    "        \n",
    "        # count misspelling\n",
    "        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n",
    "        \n",
    "        # merge prompts and summaries\n",
    "        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n",
    "\n",
    "        # after merge preprocess\n",
    "        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n",
    "        \n",
    "        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n",
    "        input_df['bigram_overlap_count'] = input_df.progress_apply(\n",
    "            self.ngram_co_occurrence,args=(2,), axis=1 \n",
    "        )\n",
    "        input_df['bigram_overlap_ratio'] = input_df['bigram_overlap_count'] / (input_df['summary_length'] - 1)\n",
    "        \n",
    "        input_df['trigram_overlap_count'] = input_df.progress_apply(\n",
    "            self.ngram_co_occurrence, args=(3,), axis=1\n",
    "        )\n",
    "        input_df['trigram_overlap_ratio'] = input_df['trigram_overlap_count'] / (input_df['summary_length'] - 2)\n",
    "        \n",
    "        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n",
    "        \n",
    "        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n",
    "    \n",
    "preprocessor = Preprocessor(model_name=CFG.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c62a033",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T07:04:33.648282Z",
     "iopub.status.busy": "2023-09-05T07:04:33.648058Z",
     "iopub.status.idle": "2023-09-05T07:04:34.535291Z",
     "shell.execute_reply": "2023-09-05T07:04:34.534386Z"
    },
    "papermill": {
     "duration": 0.898036,
     "end_time": "2023-09-05T07:04:34.537723",
     "exception": false,
     "start_time": "2023-09-05T07:04:33.639687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('/kaggle/input/train-data-fe/train_fe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e984533",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T07:04:34.553216Z",
     "iopub.status.busy": "2023-09-05T07:04:34.552998Z",
     "iopub.status.idle": "2023-09-05T07:04:34.685981Z",
     "shell.execute_reply": "2023-09-05T07:04:34.685089Z"
    },
    "papermill": {
     "duration": 0.142873,
     "end_time": "2023-09-05T07:04:34.687868",
     "exception": false,
     "start_time": "2023-09-05T07:04:34.544995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 4459.65it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 7206.71it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 3217.72it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 3245.74it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 3255.82it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 3058.19it/s]\n"
     ]
    }
   ],
   "source": [
    "test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ef1b3fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T07:04:34.707386Z",
     "iopub.status.busy": "2023-09-05T07:04:34.705851Z",
     "iopub.status.idle": "2023-09-05T07:04:34.713874Z",
     "shell.execute_reply": "2023-09-05T07:04:34.713006Z"
    },
    "papermill": {
     "duration": 0.019664,
     "end_time": "2023-09-05T07:04:34.715932",
     "exception": false,
     "start_time": "2023-09-05T07:04:34.696268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Model Function Definition\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "    return {\"rmse\": rmse}\n",
    "\n",
    "def compute_mcrmse(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculates mean columnwise root mean squared error\n",
    "    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
    "    mcrmse = np.mean(col_rmse)\n",
    "\n",
    "    return {\n",
    "        \"content_rmse\": col_rmse[0],\n",
    "        \"wording_rmse\": col_rmse[1],\n",
    "        \"mcrmse\": mcrmse,\n",
    "    }\n",
    "\n",
    "def compt_score(content_true, content_pred, wording_true, wording_pred):\n",
    "    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n",
    "    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n",
    "    \n",
    "    return (content_score + wording_score)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c309d060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T07:04:34.733629Z",
     "iopub.status.busy": "2023-09-05T07:04:34.733436Z",
     "iopub.status.idle": "2023-09-05T07:04:34.760650Z",
     "shell.execute_reply": "2023-09-05T07:04:34.759856Z"
    },
    "papermill": {
     "duration": 0.038703,
     "end_time": "2023-09-05T07:04:34.762778",
     "exception": false,
     "start_time": "2023-09-05T07:04:34.724075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContentScoreRegressor:\n",
    "    def __init__(self, \n",
    "                model_name: str,\n",
    "                model_dir: str,\n",
    "                target: str,\n",
    "                hidden_dropout_prob: float,\n",
    "                attention_probs_dropout_prob: float,\n",
    "                max_length: int,\n",
    "                ):\n",
    "        self.inputs = [\"prompt_text\", \"prompt_title\", \"prompt_question\", \"fixed_summary_text\"]\n",
    "        self.input_col = \"input\"\n",
    "        \n",
    "        self.text_cols = [self.input_col] \n",
    "        self.target = target\n",
    "        self.target_cols = [target]\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.model_dir = model_dir\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/{model_name}\")\n",
    "        self.model_config = AutoConfig.from_pretrained(f\"/kaggle/input/{model_name}\")\n",
    "        \n",
    "        self.model_config.update({\n",
    "            \"hidden_dropout_prob\": hidden_dropout_prob,\n",
    "            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n",
    "            \"num_labels\": 1,\n",
    "            \"problem_type\": \"regression\",\n",
    "        })\n",
    "        \n",
    "        seed_everything(seed=42)\n",
    "\n",
    "        self.data_collator = DataCollatorWithPadding(\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "\n",
    "\n",
    "    def tokenize_function(self, examples: pd.DataFrame):\n",
    "        labels = [examples[self.target]]\n",
    "        tokenized = self.tokenizer(examples[self.input_col],\n",
    "                         padding=False,\n",
    "                         truncation=True,\n",
    "                         max_length=self.max_length)\n",
    "        return {\n",
    "            **tokenized,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "    \n",
    "    def tokenize_function_test(self, examples: pd.DataFrame):\n",
    "        tokenized = self.tokenizer(examples[self.input_col],\n",
    "                         padding=False,\n",
    "                         truncation=True,\n",
    "                         max_length=self.max_length)\n",
    "        return tokenized\n",
    "        \n",
    "    def train(self, \n",
    "            fold: int,\n",
    "            train_df: pd.DataFrame,\n",
    "            valid_df: pd.DataFrame,\n",
    "            batch_size: int,\n",
    "            learning_rate: float,\n",
    "            weight_decay: float,\n",
    "            num_train_epochs: float,\n",
    "            save_steps: int,\n",
    "        ) -> None:\n",
    "        \"\"\"fine-tuning\"\"\"\n",
    "        \n",
    "        sep = self.tokenizer.sep_token\n",
    "        train_df[self.input_col] = (\n",
    "                    train_df[\"prompt_title\"] + sep \n",
    "                    + train_df[\"prompt_question\"] + sep \n",
    "                    + train_df[\"fixed_summary_text\"]\n",
    "                  )\n",
    "        #However, while Deberta has an upper limit of 512 tokens, the number of tokens for a simple combination of \n",
    "        #\"prompt_text\", \"prompt_title\", and \"prompt_question\" may exceed 512.\n",
    "        #Because \"prompt_text\" has a large number of tokens, in this time, \n",
    "        #use for modeling with the fields \"prompt_title\", \"prompt_question\" and \"fixed_summary_text\".\n",
    "\n",
    "        valid_df[self.input_col] = (\n",
    "                    valid_df[\"prompt_title\"] + sep \n",
    "                    + valid_df[\"prompt_question\"] + sep \n",
    "                    + valid_df[\"fixed_summary_text\"]\n",
    "                  )\n",
    "        \n",
    "        train_df = train_df[[self.input_col] + self.target_cols]\n",
    "        valid_df = valid_df[[self.input_col] + self.target_cols]\n",
    "        \n",
    "        model_content = AutoModelForSequenceClassification.from_pretrained(\n",
    "            f\"/kaggle/input/{self.model_name}\", \n",
    "            config=self.model_config\n",
    "        )\n",
    "\n",
    "        train_dataset = Dataset.from_pandas(train_df, preserve_index=False) \n",
    "        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False) \n",
    "    \n",
    "        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=False)\n",
    "        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=False)\n",
    "\n",
    "        # eg. \"bert/fold_0/\"\n",
    "        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=model_fold_dir,\n",
    "            load_best_model_at_end=True, # select best model\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            weight_decay=weight_decay,\n",
    "            report_to='none',\n",
    "            greater_is_better=False,\n",
    "            save_strategy=\"steps\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=save_steps,\n",
    "            save_steps=save_steps,\n",
    "            metric_for_best_model=\"rmse\",\n",
    "            save_total_limit=1\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model_content,\n",
    "            args=training_args,\n",
    "            train_dataset=train_tokenized_datasets,\n",
    "            eval_dataset=val_tokenized_datasets,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            data_collator=self.data_collator\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        \n",
    "        model_content.save_pretrained(self.model_dir)\n",
    "        self.tokenizer.save_pretrained(self.model_dir)\n",
    "\n",
    "        \n",
    "    def predict(self, \n",
    "                test_df: pd.DataFrame,\n",
    "                fold: int,\n",
    "               ):\n",
    "        \"\"\"predict content score\"\"\"\n",
    "        \n",
    "        sep = self.tokenizer.sep_token\n",
    "        in_text = (\n",
    "                    test_df[\"prompt_title\"] + sep \n",
    "                    + test_df[\"prompt_question\"] + sep \n",
    "                    + test_df[\"fixed_summary_text\"]\n",
    "                  )\n",
    "        test_df[self.input_col] = in_text\n",
    "\n",
    "        test_ = test_df[[self.input_col]]\n",
    "    \n",
    "        test_dataset = Dataset.from_pandas(test_, preserve_index=False) \n",
    "        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n",
    "\n",
    "        model_content = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n",
    "        model_content.eval()\n",
    "        \n",
    "        # e.g. \"bert/fold_0/\"\n",
    "        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n",
    "\n",
    "        test_args = TrainingArguments(\n",
    "            output_dir=model_fold_dir,\n",
    "            do_train = False,\n",
    "            do_predict = True,\n",
    "            per_device_eval_batch_size = 4,   \n",
    "            dataloader_drop_last = False,\n",
    "        )\n",
    "\n",
    "        # init trainer\n",
    "        infer_content = Trainer(\n",
    "                      model = model_content, \n",
    "                      tokenizer=self.tokenizer,\n",
    "                      data_collator=self.data_collator,\n",
    "                      args = test_args)\n",
    "\n",
    "        preds = infer_content.predict(test_tokenized_dataset)[0]\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae944b78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T07:04:34.780946Z",
     "iopub.status.busy": "2023-09-05T07:04:34.780756Z",
     "iopub.status.idle": "2023-09-05T07:04:34.796220Z",
     "shell.execute_reply": "2023-09-05T07:04:34.795390Z"
    },
    "papermill": {
     "duration": 0.027268,
     "end_time": "2023-09-05T07:04:34.798278",
     "exception": false,
     "start_time": "2023-09-05T07:04:34.771010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_by_fold(\n",
    "        train_df: pd.DataFrame,\n",
    "        model_name: str,\n",
    "        target:str,\n",
    "        save_each_model: bool,\n",
    "        n_splits: int,\n",
    "        batch_size: int,\n",
    "        learning_rate: int,\n",
    "        hidden_dropout_prob: float,\n",
    "        attention_probs_dropout_prob: float,\n",
    "        weight_decay: float,\n",
    "        num_train_epochs: int,\n",
    "        save_steps: int,\n",
    "        max_length:int\n",
    "    ):\n",
    "\n",
    "    # delete old model files\n",
    "    if os.path.exists(model_name):\n",
    "        shutil.rmtree(model_name)\n",
    "    \n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "    for fold in range(CFG.n_splits):\n",
    "        print(f\"fold {fold}:\")\n",
    "        \n",
    "        train_data = train_df[train_df[\"fold\"] != fold]\n",
    "        valid_data = train_df[train_df[\"fold\"] == fold]\n",
    "        \n",
    "        if save_each_model == True:\n",
    "            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n",
    "        else: \n",
    "            model_dir =  f\"{model_name}/fold_{fold}\"\n",
    "\n",
    "        csr = ContentScoreRegressor(\n",
    "            model_name=model_name,\n",
    "            target=target,\n",
    "            model_dir = model_dir, \n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            max_length=max_length,\n",
    "           )\n",
    "        \n",
    "        csr.train(\n",
    "            fold=fold,\n",
    "            train_df=train_data,\n",
    "            valid_df=valid_data, \n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            save_steps=save_steps,\n",
    "        )\n",
    "\n",
    "def validate(\n",
    "    train_df: pd.DataFrame,\n",
    "    target:str,\n",
    "    save_each_model: bool,\n",
    "    model_name: str,\n",
    "    hidden_dropout_prob: float,\n",
    "    attention_probs_dropout_prob: float,\n",
    "    max_length : int\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"predict oof data\"\"\"\n",
    "    for fold in range(CFG.n_splits):\n",
    "        print(f\"fold {fold}:\")\n",
    "        \n",
    "        valid_data = train_df[train_df[\"fold\"] == fold]\n",
    "        \n",
    "        if save_each_model == True:\n",
    "            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n",
    "        else: \n",
    "            model_dir =  f\"{model_name}/fold_{fold}\"\n",
    "        \n",
    "        csr = ContentScoreRegressor(\n",
    "            model_name=model_name,\n",
    "            target=target,\n",
    "            model_dir = model_dir,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            max_length=max_length,\n",
    "           )\n",
    "        \n",
    "        pred = csr.predict(\n",
    "            test_df=valid_data, \n",
    "            fold=fold\n",
    "        )\n",
    "        \n",
    "        train_df.loc[valid_data.index, f\"{target}_pred\"] = pred\n",
    "\n",
    "    return train_df\n",
    "    \n",
    "def predict(\n",
    "    test_df: pd.DataFrame,\n",
    "    target:str,\n",
    "    save_each_model: bool,\n",
    "    model_name: str,\n",
    "    hidden_dropout_prob: float,\n",
    "    attention_probs_dropout_prob: float,\n",
    "    max_length : int\n",
    "    ):\n",
    "    \"\"\"predict using mean folds\"\"\"\n",
    "\n",
    "    for fold in range(CFG.n_splits):\n",
    "        print(f\"fold {fold}:\")\n",
    "        \n",
    "        if save_each_model == True:\n",
    "            model_dir =  f\"{target}/{model_name}/fold_{fold}\"\n",
    "        else: \n",
    "            model_dir =  f\"{model_name}/fold_{fold}\"\n",
    "\n",
    "        csr = ContentScoreRegressor(\n",
    "            model_name=model_name,\n",
    "            target=target,\n",
    "            model_dir = model_dir, \n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            max_length=max_length,\n",
    "           )\n",
    "        \n",
    "        pred = csr.predict(\n",
    "            test_df=test_df, \n",
    "            fold=fold\n",
    "        )\n",
    "        \n",
    "        test_df[f\"{target}_pred_{fold}\"] = pred\n",
    "    \n",
    "    test_df[f\"{target}\"] = test_df[[f\"{target}_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1)\n",
    "\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4c6d024",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T07:04:34.815824Z",
     "iopub.status.busy": "2023-09-05T07:04:34.815611Z",
     "iopub.status.idle": "2023-09-05T08:50:25.728353Z",
     "shell.execute_reply": "2023-09-05T08:50:25.726920Z"
    },
    "papermill": {
     "duration": 6350.924745,
     "end_time": "2023-09-05T08:50:25.731297",
     "exception": false,
     "start_time": "2023-09-05T07:04:34.806552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2130' max='2130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2130/2130 24:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.401465</td>\n",
       "      <td>0.633612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.322824</td>\n",
       "      <td>0.568176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.378812</td>\n",
       "      <td>0.615477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.301156</td>\n",
       "      <td>0.548777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.431200</td>\n",
       "      <td>0.306915</td>\n",
       "      <td>0.553999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.431200</td>\n",
       "      <td>0.318062</td>\n",
       "      <td>0.563969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.431200</td>\n",
       "      <td>0.290505</td>\n",
       "      <td>0.538985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.431200</td>\n",
       "      <td>0.299102</td>\n",
       "      <td>0.546902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.431200</td>\n",
       "      <td>0.371577</td>\n",
       "      <td>0.609571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.250600</td>\n",
       "      <td>0.317007</td>\n",
       "      <td>0.563033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.250600</td>\n",
       "      <td>0.324798</td>\n",
       "      <td>0.569911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.250600</td>\n",
       "      <td>0.308827</td>\n",
       "      <td>0.555722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.250600</td>\n",
       "      <td>0.316933</td>\n",
       "      <td>0.562968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.250600</td>\n",
       "      <td>0.329111</td>\n",
       "      <td>0.573682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.309852</td>\n",
       "      <td>0.556644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.339435</td>\n",
       "      <td>0.582611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.336609</td>\n",
       "      <td>0.580180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.325895</td>\n",
       "      <td>0.570872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.328690</td>\n",
       "      <td>0.573315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.087400</td>\n",
       "      <td>0.334179</td>\n",
       "      <td>0.578082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.087400</td>\n",
       "      <td>0.330315</td>\n",
       "      <td>0.574730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2150' max='2150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2150/2150 26:03, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.636178</td>\n",
       "      <td>0.797607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.599568</td>\n",
       "      <td>0.774318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.244512</td>\n",
       "      <td>1.115577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.658381</td>\n",
       "      <td>0.811407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.412200</td>\n",
       "      <td>0.705865</td>\n",
       "      <td>0.840158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.412200</td>\n",
       "      <td>0.577712</td>\n",
       "      <td>0.760074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.412200</td>\n",
       "      <td>0.890642</td>\n",
       "      <td>0.943738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.412200</td>\n",
       "      <td>0.753444</td>\n",
       "      <td>0.868011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.412200</td>\n",
       "      <td>0.658319</td>\n",
       "      <td>0.811369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.745191</td>\n",
       "      <td>0.863244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.623181</td>\n",
       "      <td>0.789418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.713502</td>\n",
       "      <td>0.844690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.605331</td>\n",
       "      <td>0.778030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.579514</td>\n",
       "      <td>0.761258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.732359</td>\n",
       "      <td>0.855780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.595126</td>\n",
       "      <td>0.771444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.633275</td>\n",
       "      <td>0.795786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.659399</td>\n",
       "      <td>0.812034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.697694</td>\n",
       "      <td>0.835281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.088100</td>\n",
       "      <td>0.680865</td>\n",
       "      <td>0.825145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.088100</td>\n",
       "      <td>0.685936</td>\n",
       "      <td>0.828213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2155' max='2155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2155/2155 25:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.481597</td>\n",
       "      <td>0.693972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.444654</td>\n",
       "      <td>0.666824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.465224</td>\n",
       "      <td>0.682073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.370006</td>\n",
       "      <td>0.608281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.443300</td>\n",
       "      <td>0.383923</td>\n",
       "      <td>0.619615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.443300</td>\n",
       "      <td>0.305174</td>\n",
       "      <td>0.552426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.443300</td>\n",
       "      <td>0.347917</td>\n",
       "      <td>0.589845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.443300</td>\n",
       "      <td>0.296378</td>\n",
       "      <td>0.544406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.443300</td>\n",
       "      <td>0.339170</td>\n",
       "      <td>0.582383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.275500</td>\n",
       "      <td>0.348989</td>\n",
       "      <td>0.590753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.275500</td>\n",
       "      <td>0.310395</td>\n",
       "      <td>0.557131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.275500</td>\n",
       "      <td>0.316579</td>\n",
       "      <td>0.562654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.275500</td>\n",
       "      <td>0.343623</td>\n",
       "      <td>0.586194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.275500</td>\n",
       "      <td>0.317781</td>\n",
       "      <td>0.563721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.173800</td>\n",
       "      <td>0.391020</td>\n",
       "      <td>0.625316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.173800</td>\n",
       "      <td>0.374266</td>\n",
       "      <td>0.611773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.173800</td>\n",
       "      <td>0.313817</td>\n",
       "      <td>0.560194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.173800</td>\n",
       "      <td>0.343721</td>\n",
       "      <td>0.586277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.173800</td>\n",
       "      <td>0.357400</td>\n",
       "      <td>0.597830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>0.367177</td>\n",
       "      <td>0.605952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>0.368859</td>\n",
       "      <td>0.607338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2530' max='2530' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2530/2530 25:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.065670</td>\n",
       "      <td>1.032313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.760552</td>\n",
       "      <td>0.872096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.757904</td>\n",
       "      <td>0.870577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.657286</td>\n",
       "      <td>0.810732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.424100</td>\n",
       "      <td>0.456193</td>\n",
       "      <td>0.675421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.424100</td>\n",
       "      <td>0.513785</td>\n",
       "      <td>0.716788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.424100</td>\n",
       "      <td>0.427203</td>\n",
       "      <td>0.653608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.424100</td>\n",
       "      <td>0.459013</td>\n",
       "      <td>0.677505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.424100</td>\n",
       "      <td>0.434639</td>\n",
       "      <td>0.659272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.257800</td>\n",
       "      <td>0.427480</td>\n",
       "      <td>0.653819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.257800</td>\n",
       "      <td>0.463027</td>\n",
       "      <td>0.680461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.257800</td>\n",
       "      <td>0.431767</td>\n",
       "      <td>0.657089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.257800</td>\n",
       "      <td>0.456309</td>\n",
       "      <td>0.675507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.257800</td>\n",
       "      <td>0.505985</td>\n",
       "      <td>0.711326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>0.470666</td>\n",
       "      <td>0.686051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>0.452579</td>\n",
       "      <td>0.672740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>0.471658</td>\n",
       "      <td>0.686774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>0.449263</td>\n",
       "      <td>0.670271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>0.470862</td>\n",
       "      <td>0.686194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.455515</td>\n",
       "      <td>0.674918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.463163</td>\n",
       "      <td>0.680561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.461395</td>\n",
       "      <td>0.679261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.454294</td>\n",
       "      <td>0.674013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.463704</td>\n",
       "      <td>0.680958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.072100</td>\n",
       "      <td>0.471279</td>\n",
       "      <td>0.686497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv wording rmse: 0.6274671961378236\n"
     ]
    }
   ],
   "source": [
    "for target in [\"wording\"]:\n",
    "    train_by_fold(\n",
    "        train,\n",
    "        model_name=CFG.model_name,\n",
    "        save_each_model=True,\n",
    "        target=target,\n",
    "        learning_rate=CFG.learning_rate,\n",
    "        hidden_dropout_prob=CFG.hidden_dropout_prob,\n",
    "        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "        num_train_epochs=CFG.num_train_epochs,\n",
    "        n_splits=CFG.n_splits,\n",
    "        batch_size=CFG.batch_size,\n",
    "        save_steps=CFG.save_steps,\n",
    "        max_length=CFG.max_length\n",
    "    )\n",
    "    \n",
    "    \n",
    "    train = validate(\n",
    "        train,\n",
    "        target=target,\n",
    "        save_each_model=True,\n",
    "        model_name=CFG.model_name,\n",
    "        hidden_dropout_prob=CFG.hidden_dropout_prob,\n",
    "        attention_probs_dropout_prob=CFG.attention_probs_dropout_prob,\n",
    "        max_length=CFG.max_length\n",
    "    )\n",
    "\n",
    "    rmse = mean_squared_error(train[target], train[f\"{target}_pred\"], squared=False)\n",
    "    print(f\"cv {target} rmse: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2df393b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T08:50:25.755681Z",
     "iopub.status.busy": "2023-09-05T08:50:25.755182Z",
     "iopub.status.idle": "2023-09-05T08:50:25.782468Z",
     "shell.execute_reply": "2023-09-05T08:50:25.781366Z"
    },
    "papermill": {
     "duration": 0.042253,
     "end_time": "2023-09-05T08:50:25.785058",
     "exception": false,
     "start_time": "2023-09-05T08:50:25.742805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>fixed_summary_text</th>\n",
       "      <th>splling_err_num</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>...</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>length_ratio</th>\n",
       "      <th>word_overlap_count</th>\n",
       "      <th>bigram_overlap_count</th>\n",
       "      <th>bigram_overlap_ratio</th>\n",
       "      <th>trigram_overlap_count</th>\n",
       "      <th>trigram_overlap_ratio</th>\n",
       "      <th>quotes_count</th>\n",
       "      <th>fold</th>\n",
       "      <th>wording_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000e8c3c7ddb</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave was an experimentto see how peo...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>64</td>\n",
       "      <td>The third wave was an experimental see how peo...</td>\n",
       "      <td>5</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>...</td>\n",
       "      <td>660</td>\n",
       "      <td>0.096970</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.628423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0020ae56ffbf</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>They would rub it up with soda to make the sme...</td>\n",
       "      <td>-0.548304</td>\n",
       "      <td>0.506755</td>\n",
       "      <td>54</td>\n",
       "      <td>They would rub it up with soda to make the sme...</td>\n",
       "      <td>2</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>...</td>\n",
       "      <td>1076</td>\n",
       "      <td>0.050186</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>10</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.195068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004e978e639e</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>In Egypt, there were many occupations and soci...</td>\n",
       "      <td>3.128928</td>\n",
       "      <td>4.231226</td>\n",
       "      <td>269</td>\n",
       "      <td>In Egypt, there were many occupations and soci...</td>\n",
       "      <td>32</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>...</td>\n",
       "      <td>625</td>\n",
       "      <td>0.430400</td>\n",
       "      <td>22</td>\n",
       "      <td>52</td>\n",
       "      <td>0.194030</td>\n",
       "      <td>23</td>\n",
       "      <td>0.086142</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.003590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>005ab0199905</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>The highest class was Pharaohs these people we...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.471415</td>\n",
       "      <td>28</td>\n",
       "      <td>The highest class was Pharaohs these people we...</td>\n",
       "      <td>5</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>...</td>\n",
       "      <td>625</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>5</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.920921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0070c9e7af47</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The Third Wave developed  rapidly because the ...</td>\n",
       "      <td>3.272894</td>\n",
       "      <td>3.219757</td>\n",
       "      <td>232</td>\n",
       "      <td>The Third Wave developed  rapidly because the ...</td>\n",
       "      <td>29</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>...</td>\n",
       "      <td>660</td>\n",
       "      <td>0.351515</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>5</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.476475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id                                               text  \\\n",
       "0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n",
       "1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n",
       "2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n",
       "3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n",
       "4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n",
       "\n",
       "    content   wording  summary_length  \\\n",
       "0  0.205683  0.380538              64   \n",
       "1 -0.548304  0.506755              54   \n",
       "2  3.128928  4.231226             269   \n",
       "3 -0.210614 -0.471415              28   \n",
       "4  3.272894  3.219757             232   \n",
       "\n",
       "                                  fixed_summary_text  splling_err_num  \\\n",
       "0  The third wave was an experimental see how peo...                5   \n",
       "1  They would rub it up with soda to make the sme...                2   \n",
       "2  In Egypt, there were many occupations and soci...               32   \n",
       "3  The highest class was Pharaohs these people we...                5   \n",
       "4  The Third Wave developed  rapidly because the ...               29   \n",
       "\n",
       "                                     prompt_question  \\\n",
       "0  Summarize how the Third Wave developed over su...   \n",
       "1  Summarize the various ways the factory would u...   \n",
       "2  In complete sentences, summarize the structure...   \n",
       "3  In complete sentences, summarize the structure...   \n",
       "4  Summarize how the Third Wave developed over su...   \n",
       "\n",
       "                prompt_title  ... prompt_length  length_ratio  \\\n",
       "0             The Third Wave  ...           660      0.096970   \n",
       "1    Excerpt from The Jungle  ...          1076      0.050186   \n",
       "2  Egyptian Social Structure  ...           625      0.430400   \n",
       "3  Egyptian Social Structure  ...           625      0.044800   \n",
       "4             The Third Wave  ...           660      0.351515   \n",
       "\n",
       "   word_overlap_count  bigram_overlap_count  bigram_overlap_ratio  \\\n",
       "0                  14                     4              0.063492   \n",
       "1                  18                    22              0.415094   \n",
       "2                  22                    52              0.194030   \n",
       "3                   6                     6              0.222222   \n",
       "4                  23                    27              0.116883   \n",
       "\n",
       "   trigram_overlap_count  trigram_overlap_ratio  quotes_count  fold  \\\n",
       "0                      0               0.000000             0   3.0   \n",
       "1                     10               0.192308             0   2.0   \n",
       "2                     23               0.086142             2   1.0   \n",
       "3                      5               0.192308             0   1.0   \n",
       "4                      5               0.021739             4   3.0   \n",
       "\n",
       "   wording_pred  \n",
       "0      0.628423  \n",
       "1     -0.195068  \n",
       "2      2.003590  \n",
       "3     -0.920921  \n",
       "4      2.476475  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4d1d1aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T08:50:25.808606Z",
     "iopub.status.busy": "2023-09-05T08:50:25.808126Z",
     "iopub.status.idle": "2023-09-05T08:50:27.437347Z",
     "shell.execute_reply": "2023-09-05T08:50:27.436311Z"
    },
    "papermill": {
     "duration": 1.643398,
     "end_time": "2023-09-05T08:50:27.439744",
     "exception": false,
     "start_time": "2023-09-05T08:50:25.796346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.to_csv('train_wording.csv',index=False)"
   ]
  }
 ],
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": 3
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.6.4"
 },
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6409.57575,
   "end_time": "2023-09-05T08:50:30.933622",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-09-05T07:03:41.357872",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
